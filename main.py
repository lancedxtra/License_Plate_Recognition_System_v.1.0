import cv2
import numpy as np
import argparse
import time
import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from PIL import Image, ImageDraw, ImageFont

# å¯¼å…¥ä¸‰ä¸ªæ¨¡å—
from license_plate_detection import LicensePlateDetector
from license_plate_preprocessor import LicensePlatePreprocessor
from license_plate_ocr_engine import get_license_plate_info
try:
    from video_processor import VideoLicensePlateProcessor, create_video_processor_from_system
    VIDEO_PROCESSOR_AVAILABLE = True
except ImportError as e:
    print(f"è­¦å‘Š: è§†é¢‘å¤„ç†å™¨æ¨¡å—ä¸å¯ç”¨: {e}")
    VIDEO_PROCESSOR_AVAILABLE = False

try:
    from camera_manager import (
        list_all_cameras,
        select_camera_interactive,
        get_camera_name,
        get_camera_info,
        print_camera_info,
        find_best_camera,
        CameraManager
    )
    CAMERA_MANAGER_AVAILABLE = True
except ImportError as e:
    print(f"è­¦å‘Š: æ‘„åƒå¤´ç®¡ç†å™¨æ¨¡å—ä¸å¯ç”¨: {e}")
    print("æ‘„åƒå¤´æ£€æµ‹åŠŸèƒ½å°†å—é™")
    CAMERA_MANAGER_AVAILABLE = False

class LicensePlateSystem:
    """
    è½¦ç‰Œè¯†åˆ«ç³»ç»Ÿ - æ•´åˆæ£€æµ‹ã€çŸ«æ­£ã€é¢„å¤„ç†å’Œè¯†åˆ«ï¼ˆå«é¢œè‰²æ£€æµ‹ï¼‰
    """
    
    def __init__(self, 
                 detection_model_path: str = 'yolov8s.pt',
                 detection_conf_threshold: float = 0.5,
                 use_preprocessing: bool = True):
        """
        åˆå§‹åŒ–è½¦ç‰Œè¯†åˆ«ç³»ç»Ÿ
        
        Args:
            detection_model_path: YOLOæ£€æµ‹æ¨¡å‹è·¯å¾„
            detection_conf_threshold: æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼
            use_preprocessing: æ˜¯å¦ä½¿ç”¨é¢„å¤„ç†
        """
        print("=" * 60)
        print("åˆå§‹åŒ–è½¦ç‰Œè¯†åˆ«ç³»ç»Ÿ...")
        print("=" * 60)
        
        # åˆå§‹åŒ–ä¸‰ä¸ªæ¨¡å—
        print("1. åŠ è½½è½¦ç‰Œæ£€æµ‹å™¨...")
        self.detector = LicensePlateDetector(
            model_path=detection_model_path,
            conf_threshold=detection_conf_threshold
        )
        
        print("2. åŠ è½½è½¦ç‰Œé¢„å¤„ç†å™¨...")
        self.preprocessor = LicensePlatePreprocessor(
            target_size=(640, 480)
        )
        
        self.use_preprocessing = use_preprocessing
        
        print("âœ“ ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        print(f"  é¢„å¤„ç†: {'å¯ç”¨' if use_preprocessing else 'ç¦ç”¨'}")
        print()
    
    def draw_chinese_text(self, img, text, position, text_color, text_size=20):
        """
        ä½¿ç”¨PILç»˜åˆ¶ä¸­æ–‡æ–‡æœ¬ (å¢å¼ºç‰ˆï¼šè‡ªåŠ¨å¯»æ‰¾å­—ä½“)
        """
        if (isinstance(img, np.ndarray)):
            # OpenCVå›¾ç‰‡(BGR)è½¬æ¢ä¸ºPILå›¾ç‰‡(RGB)
            img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
            draw = ImageDraw.Draw(img_pil)
            
            # --- å­—ä½“åŠ è½½é€»è¾‘ ---
            font = None
            # å­—ä½“æŸ¥æ‰¾ä¼˜å…ˆçº§åˆ—è¡¨
            font_paths = [
                "simhei.ttf",                 # 1. ä¼˜å…ˆæ‰¾å½“å‰ç›®å½•ä¸‹çš„ simhei.ttf
                "msyh.ttf",                   # 2. æ‰¾å½“å‰ç›®å½•ä¸‹çš„ å¾®è½¯é›…é»‘
                "font.ttf",                   # 3. æ‰¾å½“å‰ç›®å½•ä¸‹çš„ font.ttf (ä½ å¯ä»¥è‡ªå·±æ”¹å)
                "C:/Windows/Fonts/simhei.ttf",# 4. Windows ç³»ç»Ÿç»å¯¹è·¯å¾„
                "C:/Windows/Fonts/msyh.ttf",  # 5. Windows ç³»ç»Ÿç»å¯¹è·¯å¾„ (å¾®è½¯é›…é»‘)
                "/usr/share/fonts/truetype/droid/DroidSansFallbackFull.ttf" # 6. Linux å¸¸è§è·¯å¾„
            ]
            
            for path in font_paths:
                if os.path.exists(path):
                    try:
                        font = ImageFont.truetype(path, text_size, encoding="utf-8")
                        break
                    except Exception as e:
                        continue
            
            # å¦‚æœæ²¡æ‰¾åˆ°ä»»ä½•ä¸­æ–‡å­—ä½“
            if font is None:
                # åªæ‰“å°ä¸€æ¬¡è­¦å‘Šï¼Œé¿å…åˆ·å±
                if not hasattr(self, '_font_warning_shown'):
                    print("\n" + "!"*60)
                    print("ã€ä¸¥é‡è­¦å‘Šã€‘æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“æ–‡ä»¶ï¼ä¸­æ–‡å°†æ— æ³•æ˜¾ç¤ºã€‚")
                    print("è¯·å°† simhei.ttf æˆ– msyh.ttf å¤åˆ¶åˆ° main.py åŒçº§ç›®å½•ä¸‹ï¼")
                    print("!"*60 + "\n")
                    self._font_warning_shown = True
                font = ImageFont.load_default() # å›é€€åˆ°ä¸æ”¯æŒä¸­æ–‡çš„é»˜è®¤å­—ä½“
            # -------------------
            
            # ç»˜åˆ¶æ–‡æœ¬ (stroke_width=1 ç»™æ–‡å­—åŠ ä¸ªé»‘è¾¹ï¼Œé˜²æ­¢åœ¨æµ…è‰²èƒŒæ™¯çœ‹ä¸æ¸…)
            try:
                draw.text(position, text, font=font, fill=text_color, stroke_width=0)
            except:
                # æ—§ç‰ˆPillowå¯èƒ½ä¸æ”¯æŒstroke_width
                draw.text(position, text, font=font, fill=text_color)
            
            # è½¬æ¢å›OpenCVæ ¼å¼
            return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)
        return img

    def process_single_plate(self, original_image: np.ndarray, 
                            plate_info: Dict, 
                            output_dir: str,
                            plate_index: int,
                            save_results: bool = True) -> Dict:
        """
        å¤„ç†å•ä¸ªè½¦ç‰Œ
        
        Args:
            original_image: åŸå§‹å›¾åƒ
            plate_info: è½¦ç‰Œä¿¡æ¯
            output_dir: è¾“å‡ºç›®å½•
            plate_index: è½¦ç‰Œç´¢å¼•
            save_results: æ˜¯å¦ä¿å­˜ç»“æœ
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        print(f"\nå¤„ç†è½¦ç‰Œ {plate_index}:")
        print(f"  æ£€æµ‹ç½®ä¿¡åº¦: {plate_info['confidence']:.3f}")
        print(f"  ä½ç½®: {plate_info['bbox']}")
        
        # è·å–çŸ«æ­£åçš„è½¦ç‰Œå›¾åƒ
        rectified_image = plate_info['rectified']
        
        if rectified_image is None or rectified_image.size == 0:
            print(f"  è­¦å‘Š: è½¦ç‰Œ {plate_index} å›¾åƒä¸ºç©ºï¼Œè·³è¿‡")
            return None
        
        # 1. é¢„å¤„ç†ï¼ˆå¢å¼ºå›¾åƒè´¨é‡ï¼‰
        preprocessed_image = rectified_image
        if self.use_preprocessing:
            print("  æ­¥éª¤1: é¢„å¤„ç†å›¾åƒ...")
            try:
                # ä½¿ç”¨é¢„å¤„ç†å™¨å¤„ç†å›¾åƒ
                preprocessed_image, preprocess_info = self.preprocessor.preprocess_with_color_recovery(
                    rectified_image,
                    detect_plate_region=True
                )
                
                # ä¿å­˜é¢„å¤„ç†å‰åçš„å¯¹æ¯”
                if save_results:
                    self._save_comparison(
                        rectified_image, 
                        preprocessed_image, 
                        output_dir, 
                        f"plate_{plate_index}_preprocess"
                    )
                
            except Exception as e:
                print(f"    é¢„å¤„ç†å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                preprocessed_image = rectified_image
        
        # 2. ä¿å­˜é¢„å¤„ç†åçš„å›¾åƒç”¨äºOCR
        temp_plate_path = None
        if save_results:
            temp_plate_path = f"{output_dir}/plate_{plate_index}_for_ocr.jpg"
            cv2.imwrite(temp_plate_path, preprocessed_image)
        
        # 3. OCRè¯†åˆ«è½¦ç‰Œä¿¡æ¯ï¼ˆåŒ…å«é¢œè‰²æ£€æµ‹ï¼‰
        print("  æ­¥éª¤2: OCRè¯†åˆ«è½¦ç‰Œï¼ˆå«é¢œè‰²æ£€æµ‹ï¼‰...")
        ocr_start = time.time()
        
        # ä½¿ç”¨é¢„å¤„ç†åçš„å›¾åƒè¿›è¡Œè¯†åˆ«
        if temp_plate_path:
            ocr_input_path = temp_plate_path
        else:
            # ä¸´æ—¶ä¿å­˜å›¾åƒç”¨äºOCR
            temp_path = f"temp_plate_{plate_index}.jpg"
            cv2.imwrite(temp_path, preprocessed_image)
            ocr_input_path = temp_path
        
        # è°ƒç”¨OCRå¼•æ“ï¼ˆåŒ…å«é¢œè‰²æ£€æµ‹ï¼‰
        ocr_result = get_license_plate_info(ocr_input_path)
        
        ocr_time = time.time() - ocr_start
        
        # 4. å¤„ç†OCRç»“æœ
        plate_text = "æœªçŸ¥"
        ocr_confidence = 0.0
        plate_type = "æœªçŸ¥"
        
        if ocr_result:
            plate_text, ocr_confidence, plate_type = ocr_result
            print(f"  âœ“ è¯†åˆ«æˆåŠŸ:")
            print(f"    è½¦ç‰Œå·ç : {plate_text}")
            print(f"    è½¦ç‰Œç±»å‹: {plate_type}")
            print(f"    è¯†åˆ«ç½®ä¿¡åº¦: {ocr_confidence:.3f}")
            print(f"    è¯†åˆ«è€—æ—¶: {ocr_time:.2f}s")
        else:
            print(f"  âœ— è¯†åˆ«å¤±è´¥")
        
        # 5. åœ¨åŸå›¾ä¸Šç»˜åˆ¶ç»“æœ
        annotated_image = self._annotate_plate(
            original_image.copy(),
            plate_info['bbox'],
            plate_text,
            plate_info['confidence'],
            ocr_confidence,
            plate_type
        )
        
        # 6. å‡†å¤‡ç»“æœ
        result = {
            'plate_id': plate_index,
            'detection_confidence': float(plate_info['confidence']),
            'bbox': plate_info['bbox'],
            'plate_text': plate_text,
            'ocr_confidence': float(ocr_confidence),
            'plate_type': plate_type,
            'rectified_image': rectified_image,
            'preprocessed_image': preprocessed_image,
            'annotated_image': annotated_image,
            'ocr_time': float(ocr_time),
        }
        
        # 7. ä¿å­˜å•ä¸ªè½¦ç‰Œç»“æœ
        if save_results:
            self._save_single_result(result, output_dir, plate_index)
        
        return result
    
    def process_image(self, image_path: str, 
                     save_results: bool = True,
                     output_dir: str = "results") -> List[Dict]:
        """
        å¤„ç†å•å¼ å›¾ç‰‡ï¼Œè¿”å›æ‰€æœ‰è½¦ç‰Œä¿¡æ¯
        
        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            save_results: æ˜¯å¦ä¿å­˜ç»“æœ
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            è½¦ç‰Œä¿¡æ¯åˆ—è¡¨
        """
        print(f"å¤„ç†å›¾ç‰‡: {image_path}")
        print("-" * 60)
        
        # è¯»å–åŸå§‹å›¾ç‰‡
        original_image = cv2.imread(image_path)
        if original_image is None:
            print(f"é”™è¯¯ï¼šæ— æ³•è¯»å–å›¾ç‰‡ {image_path}")
            return []
        
        # 1. æ£€æµ‹å¹¶çŸ«æ­£è½¦ç‰Œ
        print("æ­¥éª¤1: æ£€æµ‹å¹¶çŸ«æ­£è½¦ç‰Œ...")
        start_time = time.time()
        
        plates_info = self.detector.detect_all_and_rectify(image_path)
        
        if not plates_info:
            print("æœªæ£€æµ‹åˆ°è½¦ç‰Œ")
            return []
        
        detection_time = time.time() - start_time
        print(f"âœ“ æ£€æµ‹åˆ° {len(plates_info)} ä¸ªè½¦ç‰Œ (è€—æ—¶: {detection_time:.2f}s)")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if save_results:
            Path(output_dir).mkdir(exist_ok=True)
        
        # å¤„ç†æ¯ä¸ªæ£€æµ‹åˆ°çš„è½¦ç‰Œ
        all_results = []
        
        for i, plate_info in enumerate(plates_info):
            result = self.process_single_plate(
                original_image=original_image,
                plate_info=plate_info,
                output_dir=output_dir,
                plate_index=i+1,
                save_results=save_results
            )
            
            if result:
                result['detection_time'] = float(detection_time)
                result['total_time'] = float(detection_time + result['ocr_time'])
                all_results.append(result)
        
        # 8. ä¿å­˜åŒ…å«æ‰€æœ‰è½¦ç‰Œçš„åŸå›¾
        if save_results and all_results:
            # ä½¿ç”¨æœ€åä¸€ä¸ªè½¦ç‰Œçš„æ ‡æ³¨å›¾åƒ
            final_annotated = all_results[-1]['annotated_image']
            
            final_path = f"{output_dir}/final_annotated.jpg"
            cv2.imwrite(final_path, final_annotated)
            print(f"\nâœ“ æœ€ç»ˆæ ‡æ³¨å›¾ç‰‡å·²ä¿å­˜: {final_path}")
        
        # 9. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        self._cleanup_temp_files()
        
        # 10. æ‰“å°æ±‡æ€»ç»“æœ
        self._print_summary(all_results)
        
        # 11. ä¿å­˜JSONæ ¼å¼çš„å®Œæ•´ç»“æœ
        if save_results:
            self._save_json_results(all_results, output_dir)
        
        return all_results
    
    def _annotate_plate(self, image: np.ndarray, bbox: Tuple, 
                       plate_text: str, det_conf: float,
                       ocr_conf: float, plate_type: str) -> np.ndarray:
        """
        åœ¨åŸå›¾ä¸Šæ ‡æ³¨è½¦ç‰Œä¿¡æ¯
        """
        x1, y1, x2, y2 = bbox
        
        # æ ¹æ®è½¦ç‰Œç±»å‹é€‰æ‹©é¢œè‰²
        color_map = {
            'è“ç‰Œ': (255, 0, 0),      # è“è‰²
            'é»„ç‰Œ': (0, 255, 255),    # é»„è‰²
            'æ–°èƒ½æºç»¿ç‰Œ': (0, 255, 0), # ç»¿è‰²
            'ç™½ç‰Œ': (255, 255, 255),  # ç™½è‰²
            'é»‘ç‰Œ': (0, 0, 0),        # é»‘è‰²
            'ç™½ç‰Œ (è­¦ç”¨)': (255, 255, 255),  # ç™½è‰²
        }
        
        if plate_type in color_map:
            color = color_map[plate_type]
        else:
            color = (0, 255, 0) if plate_text != "æœªçŸ¥" else (0, 0, 255)
        
        # ç»˜åˆ¶è¾¹ç•Œæ¡†
        cv2.rectangle(image, (x1, y1), (x2, y2), color, 3)
        
        # å‡†å¤‡æ–‡æœ¬ä¿¡æ¯
        lines = []
        if plate_text != "æœªçŸ¥":
            lines.append(f"è½¦ç‰Œ: {plate_text}")
        
        lines.append(f"ç±»å‹: {plate_type}")
        lines.append(f"æ£€æµ‹: {det_conf:.2f} è¯†åˆ«: {ocr_conf:.2f}")
        
        # è®¡ç®—æ–‡æœ¬ä½ç½®
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.5
        thickness = 1
        
        # è®¡ç®—æ€»æ–‡æœ¬é«˜åº¦
        line_heights = []
        for line in lines:
            (text_width, text_height), _ = cv2.getTextSize(line, font, font_scale, thickness)
            line_heights.append(text_height)
        
        total_height = sum(line_heights) + 10 * len(lines)
        max_width = max([cv2.getTextSize(line, font, font_scale, thickness)[0][0] for line in lines])
        
        # æ–‡æœ¬èƒŒæ™¯ä½ç½®ï¼ˆåœ¨è½¦ç‰Œä¸Šæ–¹ï¼‰
        bg_x1 = x1
        bg_y1 = max(0, y1 - total_height - 5)
        bg_x2 = x1 + max_width + 20
        bg_y2 = y1 - 5
        
        # å¦‚æœä¸Šæ–¹ç©ºé—´ä¸è¶³ï¼Œæ”¾åœ¨ä¸‹æ–¹
        if bg_y1 < 0:
            bg_y1 = y2 + 5
            bg_y2 = bg_y1 + total_height
        
        # ç»˜åˆ¶æ–‡æœ¬èƒŒæ™¯
        cv2.rectangle(image, (bg_x1, bg_y1), (bg_x2, bg_y2), (0, 0, 0), -1)
        cv2.rectangle(image, (bg_x1, bg_y1), (bg_x2, bg_y2), color, 1)
        
        # ç»˜åˆ¶æ–‡æœ¬
        y_offset = bg_y1 + line_heights[0] + 5
        text_color = (255, 255, 255) if plate_type in ['é»„ç‰Œ', 'ç™½ç‰Œ', 'ç™½ç‰Œ (è­¦ç”¨)'] else (0, 255, 0)
        
        for i, line in enumerate(lines):
            cv2.putText(image, line, (bg_x1 + 10, y_offset), 
                       font, font_scale, text_color, thickness)
            y_offset += line_heights[i] + 10
        
        return image
    
    def _save_comparison(self, before: np.ndarray, after: np.ndarray, 
                        output_dir: str, name: str):
        """ä¿å­˜å¤„ç†å‰åå¯¹æ¯”å›¾"""
        if before is None or after is None:
            return
        
        h1, w1 = before.shape[:2]
        h2, w2 = after.shape[:2]
        
        max_height = max(h1, h2)
        
        # è°ƒæ•´å¤§å°
        if h1 != max_height:
            scale1 = max_height / h1
            new_w1 = int(w1 * scale1)
            resized_before = cv2.resize(before, (new_w1, max_height))
        else:
            resized_before = before
            new_w1 = w1
            
        if h2 != max_height:
            scale2 = max_height / h2
            new_w2 = int(w2 * scale2)
            resized_after = cv2.resize(after, (new_w2, max_height))
        else:
            resized_after = after
            new_w2 = w2
        
        # å¹¶æ’æ˜¾ç¤º
        combined = np.hstack((resized_before, resized_after))
        
        # æ·»åŠ æ ‡ç­¾
        cv2.putText(combined, "å¤„ç†å‰", (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        cv2.putText(combined, "å¤„ç†å", (new_w1 + 10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        
        save_path = f"{output_dir}/{name}_comparison.jpg"
        cv2.imwrite(save_path, combined)
    
    def _save_single_result(self, result: Dict, output_dir: str, plate_id: int):
        """ä¿å­˜å•ä¸ªè½¦ç‰Œç»“æœ"""
        base_path = f"{output_dir}/plate_{plate_id}"
        
        # ä¿å­˜çŸ«æ­£åçš„è½¦ç‰Œ
        if result['rectified_image'] is not None:
            cv2.imwrite(f"{base_path}_rectified.jpg", result['rectified_image'])
        
        # ä¿å­˜é¢„å¤„ç†åçš„è½¦ç‰Œ
        if result['preprocessed_image'] is not None:
            cv2.imwrite(f"{base_path}_preprocessed.jpg", result['preprocessed_image'])
        
        # ä¿å­˜æ ‡æ³¨å›¾
        if result['annotated_image'] is not None:
            cv2.imwrite(f"{base_path}_annotated.jpg", result['annotated_image'])
        
        # ä¿å­˜æ–‡æœ¬ç»“æœ
        with open(f"{base_path}_info.txt", "w", encoding="utf-8") as f:
            f.write("=" * 60 + "\n")
            f.write(f"è½¦ç‰Œ {plate_id} è¯†åˆ«ç»“æœ\n")
            f.write("=" * 60 + "\n\n")
            
            f.write("ã€åŸºæœ¬ä¿¡æ¯ã€‘\n")
            f.write(f"è½¦ç‰Œå·ç : {result['plate_text']}\n")
            f.write(f"è½¦ç‰Œç±»å‹: {result['plate_type']}\n")
            f.write(f"æ£€æµ‹ç½®ä¿¡åº¦: {result['detection_confidence']:.4f}\n")
            f.write(f"OCRç½®ä¿¡åº¦: {result['ocr_confidence']:.4f}\n")
            f.write(f"ä½ç½®åæ ‡: {result['bbox']}\n\n")
            
            f.write("ã€æ—¶é—´ç»Ÿè®¡ã€‘\n")
            if 'detection_time' in result:
                f.write(f"æ£€æµ‹è€—æ—¶: {result['detection_time']:.4f}s\n")
            f.write(f"è¯†åˆ«è€—æ—¶: {result['ocr_time']:.4f}s\n")
            if 'total_time' in result:
                f.write(f"æ€»è€—æ—¶: {result['total_time']:.4f}s\n\n")
    
    def _cleanup_temp_files(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        import glob
        temp_files = glob.glob("temp_plate_*.jpg") + glob.glob("temp_frame_*.jpg")
        for temp_file in temp_files:
            try:
                os.remove(temp_file)
            except:
                pass
    
    def _save_json_results(self, results: List[Dict], output_dir: str):
        """ä¿å­˜JSONæ ¼å¼çš„å®Œæ•´ç»“æœ"""
        serializable_results = []
        
        for result in results:
            # åŸºæœ¬ä¿¡æ¯çš„å¯åºåˆ—åŒ–ç‰ˆæœ¬
            serializable_result = {
                'plate_id': result['plate_id'],
                'plate_text': result['plate_text'],
                'plate_type': result['plate_type'],
                'detection_confidence': result['detection_confidence'],
                'ocr_confidence': result['ocr_confidence'],
                'bbox': result['bbox'],
            }
            
            # æ·»åŠ æ—¶é—´ä¿¡æ¯
            if 'detection_time' in result:
                serializable_result['detection_time'] = result['detection_time']
            if 'ocr_time' in result:
                serializable_result['ocr_time'] = result['ocr_time']
            if 'total_time' in result:
                serializable_result['total_time'] = result['total_time']
            
            serializable_results.append(serializable_result)
        
        # ä¿å­˜JSONæ–‡ä»¶
        json_path = f"{output_dir}/results.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ“ JSONç»“æœå·²ä¿å­˜: {json_path}")
    
    def _print_summary(self, results: List[Dict]):
        """æ‰“å°æ±‡æ€»ç»“æœ"""
        if not results:
            return
        
        print("\n" + "=" * 60)
        print("è½¦ç‰Œè¯†åˆ«æ±‡æ€»ç»“æœ")
        print("=" * 60)
        
        total_detected = len(results)
        total_recognized = sum(1 for r in results if r['plate_text'] != "æœªçŸ¥")
        
        print(f"æ£€æµ‹åˆ°è½¦ç‰Œæ€»æ•°: {total_detected}")
        print(f"æˆåŠŸè¯†åˆ«è½¦ç‰Œæ•°: {total_recognized}")
        print(f"è¯†åˆ«æˆåŠŸç‡: {total_recognized/total_detected*100:.1f}%")
        
        # é¢œè‰²åˆ†å¸ƒç»Ÿè®¡
        color_distribution = {}
        for result in results:
            color = result['plate_type']
            if color not in color_distribution:
                color_distribution[color] = 0
            color_distribution[color] += 1
        
        print("\nè½¦ç‰Œé¢œè‰²åˆ†å¸ƒ:")
        for color, count in color_distribution.items():
            percentage = count / total_detected * 100
            print(f"  {color}: {count}ä¸ª ({percentage:.1f}%)")
        
        print("\nå„è½¦ç‰Œè¯¦ç»†ç»“æœ:")
        print("-" * 60)
        for result in results:
            status = "âœ“" if result['plate_text'] != "æœªçŸ¥" else "âœ—"
            print(f"{status} è½¦ç‰Œ {result['plate_id']}:")
            print(f"  å·ç : {result['plate_text']}")
            print(f"  ç±»å‹: {result['plate_type']}")
            print(f"  æ£€æµ‹ç½®ä¿¡åº¦: {result['detection_confidence']:.4f}")
            print(f"  OCRç½®ä¿¡åº¦: {result['ocr_confidence']:.4f}")
            print()
        
        # æ—¶é—´ç»Ÿè®¡
        if 'total_time' in results[0]:
            total_time = sum(r['total_time'] for r in results)
            avg_time_per_plate = total_time / total_detected if total_detected > 0 else 0
            
            print(f"æ—¶é—´ç»Ÿè®¡:")
            print(f"  æ€»å¤„ç†æ—¶é—´: {total_time:.4f}s")
            print(f"  å¹³å‡æ¯ä¸ªè½¦ç‰Œ: {avg_time_per_plate:.4f}s")
        
        print("=" * 60)
        
    def start_camera_detection(self, 
                              camera_index: int = 0,
                              frame_width: int = 1280,
                              frame_height: int = 720,
                              fps: int = 30,
                              detection_interval: int = 10,
                              output_dir: str = "camera_results"):
        """
        å¯åŠ¨æ‘„åƒå¤´å®æ—¶æ£€æµ‹
        
        Args:
            camera_index: æ‘„åƒå¤´ç´¢å¼•
            frame_width: å¸§å®½åº¦
            frame_height: å¸§é«˜åº¦
            fps: å¸§ç‡
            detection_interval: æ£€æµ‹é—´éš”å¸§æ•°
            output_dir: è¾“å‡ºç›®å½•
        """
        print("=" * 60)
        print("å¯åŠ¨æ‘„åƒå¤´å®æ—¶æ£€æµ‹æ¨¡å¼")
        print("=" * 60)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # === æ–°å¢ï¼šå®šä¹‰æ—¥å¿—æ–‡ä»¶ ===
        log_file_path = output_path / "detection_log.csv"
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå†™å…¥è¡¨å¤´
        if not log_file_path.exists():
            with open(log_file_path, "w", encoding="utf-8-sig") as f:
                f.write("æ—¶é—´,è½¦ç‰Œå·,ç±»å‹,ç½®ä¿¡åº¦\n")
        print(f"æ—¥å¿—å°†ä¿å­˜è‡³: {log_file_path}")
        # ========================
        
        # æ‰“å¼€æ‘„åƒå¤´
        cap = cv2.VideoCapture(camera_index)
        if not cap.isOpened():
            print(f"é”™è¯¯ï¼šæ— æ³•æ‰“å¼€æ‘„åƒå¤´ {camera_index}")
            return False
        
        # è®¾ç½®æ‘„åƒå¤´å‚æ•°
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, frame_width)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, frame_height)
        cap.set(cv2.CAP_PROP_FPS, fps)
        
        # è·å–å®é™…å‚æ•°
        actual_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        actual_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        actual_fps = int(cap.get(cv2.CAP_PROP_FPS))
        
        print(f"æ‘„åƒå¤´å‚æ•°: {actual_width}x{actual_height} @ {actual_fps}fps")
        print(f"æ£€æµ‹é—´éš”: æ¯{detection_interval}å¸§æ£€æµ‹ä¸€æ¬¡")
        print("\næ§åˆ¶è¯´æ˜:")
        print("  Q: é€€å‡º")
        print("  S: ä¿å­˜å½“å‰å¸§")
        print("  P: æš‚åœ/ç»§ç»­æ£€æµ‹")
        print("  C: æ¸…ç©ºæ£€æµ‹ç»“æœ")
        print("=" * 60)
        
        frame_count = 0
        detection_count = 0
        is_paused = False
        last_detections = []
        start_time = time.time()
        
        try:
            while True:
                if not is_paused:
                    # è¯»å–å¸§
                    ret, frame = cap.read()
                    if not ret:
                        print("æ‘„åƒå¤´è¯»å–å¤±è´¥")
                        break
                    
                    frame_count += 1
                    display_frame = frame.copy()
                    
                    # æ£€æµ‹è½¦ç‰Œ
                    if frame_count % detection_interval == 0:
                        detection_count += 1
                        
                        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶ç”¨äºæ£€æµ‹
                        temp_path = f"temp_camera_frame_{detection_count}.jpg"
                        cv2.imwrite(temp_path, frame)
                        
                        try:
                            # æ£€æµ‹è½¦ç‰Œ
                            plates_info = self.detector.detect_all_and_rectify(temp_path)
                            
                            if plates_info:
                                for i, plate_info in enumerate(plates_info):
                                    # å¤„ç†æ£€æµ‹åˆ°çš„è½¦ç‰Œ
                                    result = self._process_camera_detection(frame, plate_info, i)
                                    # === æ–°å¢ï¼šåªæœ‰è¯†åˆ«æˆåŠŸæ‰ä¿å­˜åˆ°æ–‡ä»¶ ===
                                    plate_text = result.get('plate_text', 'æœªçŸ¥')
                                    if plate_text != "æœªçŸ¥":
                                        # è·å–å½“å‰ä¿¡æ¯
                                        current_time = time.strftime("%Y-%m-%d %H:%M:%S")
                                        p_type = result.get('plate_type', 'æœªçŸ¥')
                                        conf = result.get('ocr_confidence', 0)
                                        
                                        # 1. æ§åˆ¶å°åªæ‰“å°æœ‰æ•ˆçš„
                                        print(f"[{current_time}] ğŸŸ¢ æ•è·è½¦ç‰Œ: {plate_text} | {p_type} | conf:{conf:.2f}")
                                        
                                        # 2. å†™å…¥æ–‡ä»¶ (è¿½åŠ æ¨¡å¼ 'a')
                                        # ä¸ºäº†é˜²æ­¢åŒä¸€ç§’å†…é‡å¤å†™å…¥ç›¸åŒè½¦ç‰Œï¼Œå¯ä»¥åŠ ä¸ªç®€å•çš„å»é‡é€»è¾‘ï¼ˆå¯é€‰ï¼‰
                                        with open(log_file_path, "a", encoding="utf-8-sig") as f:
                                            f.write(f"{current_time},{plate_text},{p_type},{conf:.2f}\n")
                                    # =======================================

                                    last_detections.append({
                                        'result': result,
                                        'frame': frame.copy(),
                                        'timestamp': time.time()
                                    })
                                    
                                    # é™åˆ¶ä¿å­˜çš„æ•°é‡
                                    if len(last_detections) > 10:
                                        last_detections.pop(0)
                                    
                                    # æ˜¾ç¤ºç»“æœ
                                    if result.get('plate_text') != "æœªçŸ¥":
                                        print(f"æ£€æµ‹åˆ°è½¦ç‰Œ: {result['plate_text']} ({result.get('plate_type', 'æœªçŸ¥')})")
                        
                        except Exception as e:
                            print(f"æ£€æµ‹å‡ºé”™: {e}")
                        
                        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                        try:
                            os.remove(temp_path)
                        except:
                            pass
                    
                    # æ˜¾ç¤ºæœ€è¿‘çš„æ£€æµ‹ç»“æœ
                    for detection in last_detections:
                        if time.time() - detection['timestamp'] < 5.0:  # åªæ˜¾ç¤º5ç§’å†…çš„ç»“æœ
                            display_frame = self._annotate_camera_frame(display_frame, detection['result'])
                    
                    # è®¡ç®—å¹¶æ˜¾ç¤ºFPS
                    elapsed_time = time.time() - start_time
                    current_fps = frame_count / elapsed_time if elapsed_time > 0 else 0
                    
                    # === ä¿®æ”¹å¼€å§‹: ä½¿ç”¨ä¸­æ–‡ç»˜åˆ¶ ===
                    # ç»˜åˆ¶ FPS (é»„è‰²)
                    display_frame = self.draw_chinese_text(display_frame, f"FPS: {current_fps:.1f}", (10, 10), (255, 255, 0), 25)
                    # ç»˜åˆ¶ å¸§æ•° (é»„è‰²)
                    display_frame = self.draw_chinese_text(display_frame, f"å¸§æ•°: {frame_count}", (10, 40), (255, 255, 0), 25)
                    # ç»˜åˆ¶ æ£€æµ‹æ¬¡æ•° (é»„è‰²)
                    display_frame = self.draw_chinese_text(display_frame, f"æ£€æµ‹æ¬¡æ•°: {detection_count}", (10, 70), (255, 255, 0), 25)
                    
                    if is_paused:
                        center_x = display_frame.shape[1] // 2 - 80
                        display_frame = self.draw_chinese_text(display_frame, "å·²æš‚åœ", (center_x, 50), (255, 0, 0), 50)
                    # === ä¿®æ”¹ç»“æŸ ===
                
                else:
                    # æš‚åœæ—¶æ˜¾ç¤ºæœ€åä¸€å¸§
                    if 'display_frame' in locals():
                        cv2.putText(display_frame, "å·²æš‚åœ", (display_frame.shape[1]//2-50, 50),
                                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)
                
                # æ˜¾ç¤ºå›¾åƒ
                cv2.imshow('LPR System - Realtime', display_frame)
                
                # é”®ç›˜æ§åˆ¶
                key = cv2.waitKey(1) & 0xFF
                
                if key == ord('q'):  # é€€å‡º
                    break
                elif key == ord('s'):  # ä¿å­˜å½“å‰å¸§
                    timestamp = time.strftime("%Y%m%d_%H%M%S")
                    filename = output_path / f"snapshot_{timestamp}.jpg"
                    cv2.imwrite(str(filename), frame)
                    print(f"å·²ä¿å­˜æˆªå›¾: {filename}")
                elif key == ord('p'):  # æš‚åœ/ç»§ç»­
                    is_paused = not is_paused
                    print("å·²æš‚åœ" if is_paused else "å·²ç»§ç»­")
                elif key == ord('c'):  # æ¸…ç©ºæ£€æµ‹ç»“æœ
                    last_detections.clear()
                    print("å·²æ¸…ç©ºæ£€æµ‹ç»“æœ")
        
        except KeyboardInterrupt:
            print("\nç”¨æˆ·ä¸­æ–­")
        except Exception as e:
            print(f"è¿è¡Œæ—¶å‡ºé”™: {e}")
        finally:
            cap.release()
            cv2.destroyAllWindows()
            
            # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
            self._save_camera_statistics(frame_count, detection_count, elapsed_time, output_path)

    def _process_camera_detection(self, frame: np.ndarray, plate_info: Dict, plate_index: int) -> Dict:
        """å¤„ç†æ‘„åƒå¤´æ£€æµ‹åˆ°çš„è½¦ç‰Œ"""
        result = {
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
            'detection_confidence': plate_info['confidence'],
            'bbox': plate_info['bbox'],
            'plate_index': plate_index,
        }
        
        # è·å–çŸ«æ­£åçš„è½¦ç‰Œå›¾åƒ
        rectified_image = plate_info['rectified']
        if rectified_image is not None and rectified_image.size > 0:
            # é¢„å¤„ç†
            try:
                preprocessed_image = rectified_image
                if self.use_preprocessing:
                    preprocessed_image, _ = self.preprocessor.preprocess_with_color_recovery(
                        rectified_image,
                        detect_plate_region=True
                    )
                
                # ä¿å­˜ä¸´æ—¶æ–‡ä»¶ç”¨äºOCR
                temp_path = f"temp_plate_{int(time.time())}_{plate_index}.jpg"
                cv2.imwrite(temp_path, preprocessed_image)
                
                # OCRè¯†åˆ«
                ocr_result = get_license_plate_info(temp_path)
                
                if ocr_result:
                    plate_text, ocr_confidence, plate_type = ocr_result
                    result.update({
                        'plate_text': plate_text,
                        'ocr_confidence': ocr_confidence,
                        'plate_type': plate_type,
                        'ocr_success': True,
                    })
                else:
                    result['ocr_success'] = False
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    os.remove(temp_path)
                except:
                    pass
                
            except Exception as e:
                result['ocr_success'] = False
                result['error'] = str(e)
        else:
            result['ocr_success'] = False
        
        return result

    def _annotate_camera_frame(self, frame: np.ndarray, result: Dict) -> np.ndarray:
        """åœ¨æ‘„åƒå¤´å¸§ä¸Šæ ‡æ³¨æ£€æµ‹ç»“æœ (æ”¯æŒä¸­æ–‡)"""
        if 'bbox' not in result:
            return frame
        
        x1, y1, x2, y2 = result['bbox']
        
        # æ ¹æ®OCRç»“æœé€‰æ‹©é¢œè‰²
        if result.get('ocr_success', False):
            color = (0, 255, 0)  # ç»¿è‰²
            # PILé¢œè‰²æ ¼å¼æ˜¯RGBï¼ŒOpenCVæ˜¯BGRï¼Œæ‰€ä»¥è¿™é‡Œè½¬æ¢ä¸€ä¸‹ç»™æ–‡å­—ç”¨
            text_color = (0, 255, 0) 
        else:
            color = (0, 0, 255)  # çº¢è‰²
            text_color = (255, 0, 0)
        
        # ç»˜åˆ¶è¾¹ç•Œæ¡† (çŸ©å½¢æ¡†è¿˜æ˜¯ç”¨OpenCVç”»æ¯”è¾ƒå¿«)
        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
        
        # å‡†å¤‡æ–‡æœ¬ä¿¡æ¯
        text_lines = []
        if 'plate_text' in result and result['plate_text'] != "æœªçŸ¥":
            text_lines.append(f"è½¦ç‰Œ: {result['plate_text']}")
            if 'plate_type' in result:
                text_lines.append(f"ç±»å‹: {result['plate_type']}")
            if 'ocr_confidence' in result:
                text_lines.append(f"ç½®ä¿¡åº¦: {result['ocr_confidence']:.2f}")
        
        text_lines.append(f"æ£€æµ‹: {result['detection_confidence']:.2f}")
        
        # è®¡ç®—æ–‡æœ¬ä½ç½®å’ŒèƒŒæ™¯
        font_size = 20
        line_height = font_size + 5
        total_height = len(text_lines) * line_height + 10
        max_width = 200 # ä¼°ç®—å®½åº¦
        
        # æ–‡æœ¬èƒŒæ™¯ä½ç½®
        bg_x1 = x1
        bg_y1 = max(0, y1 - total_height - 10)
        bg_x2 = x1 + max_width
        bg_y2 = y1 - 5
        
        # å¦‚æœä¸Šæ–¹ç©ºé—´ä¸è¶³ï¼Œæ”¾åœ¨ä¸‹æ–¹
        if bg_y1 < 0:
            bg_y1 = y2 + 5
            bg_y2 = bg_y1 + total_height + 10
        
        # ç»˜åˆ¶åŠé€æ˜é»‘è‰²èƒŒæ™¯
        # ä½¿ç”¨åˆ‡ç‰‡æ–¹å¼æ¯”cv2.rectangleç»˜åˆ¶åŠé€æ˜æ›´å¿«
        overlay = frame.copy()
        cv2.rectangle(overlay, (bg_x1, bg_y1), (bg_x2, bg_y2), (0, 0, 0), -1)
        alpha = 0.6
        frame = cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0)
        
        # ç»˜åˆ¶è¾¹æ¡†
        cv2.rectangle(frame, (bg_x1, bg_y1), (bg_x2, bg_y2), color, 1)
        
        # ç»˜åˆ¶ä¸­æ–‡æ–‡æœ¬
        y_offset = bg_y1 + 5
        for line in text_lines:
            # æ–‡å­—é¢œè‰²ä½¿ç”¨ç™½è‰²ï¼Œçœ‹èµ·æ¥æ›´æ¸…æ™°
            frame = self.draw_chinese_text(frame, line, (bg_x1 + 5, y_offset), (255, 255, 255), font_size)
            y_offset += line_height
        
        return frame
    
    def _save_camera_statistics(self, frame_count, detection_count, elapsed_time, output_path):
        """ä¿å­˜æ‘„åƒå¤´ç»Ÿè®¡ä¿¡æ¯"""
        stats_file = output_path / "statistics.txt"
        
        with open(stats_file, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write("è½¦ç‰Œè¯†åˆ«æ‘„åƒå¤´æ£€æµ‹ç»Ÿè®¡\n")
            f.write("=" * 60 + "\n\n")
            
            f.write(f"ç»Ÿè®¡æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ€»è¿è¡Œæ—¶é—´: {elapsed_time:.2f}ç§’\n")
            f.write(f"æ€»å¸§æ•°: {frame_count}\n")
            f.write(f"æ£€æµ‹æ¬¡æ•°: {detection_count}\n")
            
            if elapsed_time > 0:
                avg_fps = frame_count / elapsed_time
                f.write(f"å¹³å‡FPS: {avg_fps:.2f}\n")
            
            f.write("\nç³»ç»Ÿé…ç½®:\n")
            f.write(f"é¢„å¤„ç†: {'å¯ç”¨' if self.use_preprocessing else 'ç¦ç”¨'}\n")
            f.write(f"æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼: {self.detector.conf_threshold}\n")
        
        print(f"ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {stats_file}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="è½¦ç‰Œè¯†åˆ«ç³»ç»Ÿï¼ˆæ”¯æŒæ‘„åƒå¤´å®æ—¶æ£€æµ‹ï¼‰")
    
    # è¾“å…¥æºé€‰æ‹©
    input_group = parser.add_mutually_exclusive_group()
    input_group.add_argument("--image", type=str, help="å¤„ç†å•å¼ å›¾ç‰‡")
    input_group.add_argument("--video", type=str, help="å¤„ç†è§†é¢‘æ–‡ä»¶")
    input_group.add_argument("--camera", action="store_true", help="å¯åŠ¨æ‘„åƒå¤´å®æ—¶æ£€æµ‹")
    input_group.add_argument("--batch", type=str, help="æ‰¹é‡å¤„ç†å›¾ç‰‡ç›®å½•")
    
    # æ‘„åƒå¤´é€‰æ‹©
    parser.add_argument("--list-cameras", action="store_true",
                       help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ‘„åƒå¤´")
    parser.add_argument("--interactive", action="store_true",
                       help="äº¤äº’å¼é€‰æ‹©æ‘„åƒå¤´")
    parser.add_argument("--camera-info", type=int, 
                       help="æ˜¾ç¤ºæ‘„åƒå¤´è¯¦ç»†ä¿¡æ¯")
    parser.add_argument("--test-camera", type=int,
                       help="æµ‹è¯•æŒ‡å®šæ‘„åƒå¤´")
    parser.add_argument("--find-best-camera", action="store_true",
                       help="å¯»æ‰¾æœ€ä½³æ‘„åƒå¤´")
    
    # è§†é¢‘å‚æ•°
    parser.add_argument("--video-start", type=float, default=0, 
                       help="è§†é¢‘å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰")
    parser.add_argument("--video-duration", type=float, default=0, 
                       help="è§†é¢‘å¤„ç†æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œ0è¡¨ç¤ºæ•´ä¸ªè§†é¢‘")
    parser.add_argument("--video-output-fps", type=float, default=0, 
                       help="è¾“å‡ºè§†é¢‘å¸§ç‡ï¼Œ0è¡¨ç¤ºä¸åŸè§†é¢‘ç›¸åŒ")
    parser.add_argument("--max-frames", type=int, default=0, 
                       help="æœ€å¤§å¤„ç†å¸§æ•°ï¼Œ0è¡¨ç¤ºæ— é™åˆ¶")
    parser.add_argument("--no-save", action="store_true", 
                       help="ä¸ä¿å­˜å¤„ç†ç»“æœï¼ˆä»…æ˜¾ç¤ºï¼‰")
    
    # æ‘„åƒå¤´å‚æ•°
    parser.add_argument("--camera-index", type=int, default=0, 
                       help="æ‘„åƒå¤´ç´¢å¼•ï¼ˆé»˜è®¤: 0ï¼‰")
    parser.add_argument("--frame-width", type=int, default=1280, 
                       help="å¸§å®½åº¦ï¼ˆé»˜è®¤: 1280ï¼‰")
    parser.add_argument("--frame-height", type=int, default=720, 
                       help="å¸§é«˜åº¦ï¼ˆé»˜è®¤: 720ï¼‰")
    parser.add_argument("--fps", type=int, default=30, 
                       help="å¸§ç‡ï¼ˆé»˜è®¤: 30ï¼‰")
    parser.add_argument("--detection-interval", type=int, default=10, 
                       help="æ£€æµ‹é—´éš”å¸§æ•°ï¼ˆé»˜è®¤: 10ï¼‰")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--model", type=str, default="yolov8s.pt", 
                       help="YOLOæ¨¡å‹è·¯å¾„ï¼ˆé»˜è®¤: yolov8s.ptï¼‰")
    parser.add_argument("--conf", type=float, default=0.5, 
                       help="æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆé»˜è®¤: 0.5ï¼‰")
    parser.add_argument("--iou", type=float, default=0.45, 
                       help="NMS IoUé˜ˆå€¼ï¼ˆé»˜è®¤: 0.45ï¼‰")
    
    # å¤„ç†å‚æ•°
    parser.add_argument("--no-preprocess", action="store_true", 
                       help="ç¦ç”¨é¢„å¤„ç†")
    parser.add_argument("--save-all", action="store_true", 
                       help="ä¿å­˜æ‰€æœ‰ä¸­é—´ç»“æœ")
    parser.add_argument("--no-display", action="store_true", 
                       help="ä¸æ˜¾ç¤ºå®æ—¶ç”»é¢")
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument("--output-dir", type=str, default="results", 
                       help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: resultsï¼‰")
    parser.add_argument("--output-format", type=str, default="jpg", 
                       choices=["jpg", "png", "bmp"], help="è¾“å‡ºå›¾ç‰‡æ ¼å¼")
    parser.add_argument("--save-json", action="store_true", 
                       help="ä¿å­˜JSONæ ¼å¼ç»“æœ")
    parser.add_argument("--save-txt", action="store_true", 
                       help="ä¿å­˜TXTæ ¼å¼ç»“æœ")
    
    # æ€§èƒ½å‚æ•°
    parser.add_argument("--device", type=str, default="cpu", 
                       choices=["cpu", "cuda", "mps"], 
                       help="è¿è¡Œè®¾å¤‡ï¼ˆé»˜è®¤: cpuï¼‰")
    parser.add_argument("--workers", type=int, default=4, 
                       help="æ•°æ®åŠ è½½çº¿ç¨‹æ•°ï¼ˆé»˜è®¤: 4ï¼‰")
    parser.add_argument("--half", action="store_true", 
                       help="ä½¿ç”¨åŠç²¾åº¦æ¨ç†ï¼ˆFP16ï¼‰")
    
    # è°ƒè¯•å‚æ•°
    parser.add_argument("--debug", action="store_true", 
                       help="å¯ç”¨è°ƒè¯•æ¨¡å¼")
    parser.add_argument("--verbose", action="store_true", 
                       help="æ˜¾ç¤ºè¯¦ç»†è¾“å‡º")
    parser.add_argument("--version", action="version", 
                       version="è½¦ç‰Œè¯†åˆ«ç³»ç»Ÿ v1.0.0")
    
    args = parser.parse_args()
    
    # æ‰“å°æ¬¢è¿ä¿¡æ¯
    print("=" * 60)
    print("è½¦ç‰Œè¯†åˆ«ç³»ç»Ÿ v1.0.0")
    print("=" * 60)

    # å¤„ç†æ‘„åƒå¤´ç›¸å…³å‚æ•°
    if args.list_cameras:
        if CAMERA_MANAGER_AVAILABLE:
            list_all_cameras()
        else:
            print("æ‘„åƒå¤´ç®¡ç†å™¨æ¨¡å—ä¸å¯ç”¨")
        return
    
    if args.camera_info is not None:
        if CAMERA_MANAGER_AVAILABLE:
            info = get_camera_info(args.camera_info)
            print_camera_info(info)
        else:
            print("æ‘„åƒå¤´ç®¡ç†å™¨æ¨¡å—ä¸å¯ç”¨")
        return
    
    if args.test_camera is not None:
        if CAMERA_MANAGER_AVAILABLE:
            from camera_manager import test_camera
            test_camera(args.test_camera)
        else:
            print("æ‘„åƒå¤´ç®¡ç†å™¨æ¨¡å—ä¸å¯ç”¨")
        return
    
    if args.find_best_camera:
        if CAMERA_MANAGER_AVAILABLE:
            best_camera = find_best_camera()
            if best_camera:
                print(f"æœ€ä½³æ‘„åƒå¤´: ç´¢å¼• {best_camera['index']}")
                print(f"åˆ†è¾¨ç‡: {best_camera['width']}x{best_camera['height']}")
                print(f"å¸§ç‡: {best_camera['fps']:.1f}fps")
            else:
                print("æœªæ‰¾åˆ°æ‘„åƒå¤´")
        else:
            print("æ‘„åƒå¤´ç®¡ç†å™¨æ¨¡å—ä¸å¯ç”¨")
        return

    # æ£€æŸ¥å‚æ•°ç»„åˆ
    if not any([args.image, args.video, args.camera, args.batch]):
        run_interactive_menu(args)
        return
    
    # åˆ›å»ºç³»ç»Ÿ
    try:
        system = LicensePlateSystem(
            detection_model_path=args.model,
            detection_conf_threshold=args.conf,
            use_preprocessing=not args.no_preprocess
        )
    except Exception as e:
        print(f"ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥:")
        print("  1. æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
        print("  2. ä¾èµ–åº“æ˜¯å¦å®‰è£…")
        print("  3. æ˜¯å¦æ­£ç¡®é…ç½®äº†CUDAï¼ˆå¦‚éœ€GPUåŠ é€Ÿï¼‰")
        return
    
    # æ ¹æ®è¾“å…¥æºå¤„ç†
    if args.image:
        # å¤„ç†å•å¼ å›¾ç‰‡
        process_image_mode(system, args)
    
    elif args.camera:
        # å¤„ç†æ‘„åƒå¤´å®æ—¶æ£€æµ‹
        process_camera_mode(system, args)
    
    elif args.video:
        # å¤„ç†è§†é¢‘æ–‡ä»¶
        process_video_mode(system, args)
    
    elif args.batch:
        # æ‰¹é‡å¤„ç†å›¾ç‰‡
        process_batch_mode(system, args)


def run_interactive_menu(args):
    """è¿è¡Œäº¤äº’å¼èœå•ï¼ˆå¯é‡å¤é€‰æ‹©ï¼‰"""
    while True:
        print("\n" + "=" * 60)
        print("è½¦ç‰Œè¯†åˆ«ç³»ç»Ÿ - äº¤äº’æ¨¡å¼")
        print("=" * 60)
        print("è¯·é€‰æ‹©æ¨¡å¼:")
        print("  1. å¤„ç†å•å¼ å›¾ç‰‡")
        print("  2. å¤„ç†è§†é¢‘æ–‡ä»¶")
        print("  3. æ‘„åƒå¤´å®æ—¶æ£€æµ‹")
        print("  4. æ‰¹é‡å¤„ç†å›¾ç‰‡ç›®å½•")
        print("  5. æ‘„åƒå¤´ç®¡ç†")
        print("  6. è¿è¡Œç³»ç»Ÿæµ‹è¯•")
        print("  0. é€€å‡º")
        print("  M. è¿”å›ä¸»èœå•ï¼ˆé‡æ–°é€‰æ‹©æ¨¡å¼ï¼‰")
        print("  H. æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯")
        print("=" * 60)
        
        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (0-6, M, H): ").strip().lower()
        
        if choice == "0":
            print("é€€å‡ºç³»ç»Ÿ")
            break
        
        elif choice == "1":
            handle_image_mode(args)
            input("\næŒ‰ Enter é”®è¿”å›èœå•...")
        
        elif choice == "2":
            handle_video_mode(args)
            input("\næŒ‰ Enter é”®è¿”å›èœå•...")
        
        elif choice == "3":
            handle_camera_mode(args)
            input("\næŒ‰ Enter é”®è¿”å›èœå•...")
        
        elif choice == "4":
            handle_batch_mode(args)
            input("\næŒ‰ Enter é”®è¿”å›èœå•...")
        
        elif choice == "5":
            handle_camera_management_mode()
            continue  # ç»§ç»­æ˜¾ç¤ºæ‘„åƒå¤´ç®¡ç†å­èœå•
        
        elif choice == "6":
            run_tests()
            input("\næŒ‰ Enter é”®è¿”å›èœå•...")
        
        elif choice == "m":
            continue  # ç»§ç»­å¾ªç¯ï¼Œæ˜¾ç¤ºä¸»èœå•
        
        elif choice == "h":
            print_help_info()
            input("\næŒ‰ Enter é”®è¿”å›èœå•...")
        
        else:
            print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")
            input("\næŒ‰ Enter é”®è¿”å›èœå•...")


def handle_camera_management_mode():
    """æ‘„åƒå¤´ç®¡ç†å­èœå•"""
    while True:
        print("\n" + "=" * 60)
        print("æ‘„åƒå¤´ç®¡ç†")
        print("=" * 60)
        print("è¯·é€‰æ‹©åŠŸèƒ½:")
        print("  1. åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ‘„åƒå¤´")
        print("  2. æŸ¥çœ‹æ‘„åƒå¤´è¯¦ç»†ä¿¡æ¯")
        print("  3. æµ‹è¯•æ‘„åƒå¤´")
        print("  4. äº¤äº’å¼é€‰æ‹©æ‘„åƒå¤´")
        print("  5. å¯»æ‰¾æœ€ä½³æ‘„åƒå¤´")
        print("  6. æ‘„åƒå¤´å®æ—¶é¢„è§ˆ")
        print("  0. è¿”å›ä¸»èœå•")
        print("  B. è¿”å›ä¸Šä¸€å±‚ï¼ˆä¸»èœå•ï¼‰")
        print("=" * 60)
        
        sub_choice = input("\nè¯·è¾“å…¥é€‰æ‹© (0-6, B): ").strip().lower()
        
        if sub_choice == "0" or sub_choice == "b":
            print("è¿”å›ä¸»èœå•")
            break
        
        elif sub_choice == "1":
            if CAMERA_MANAGER_AVAILABLE:
                list_all_cameras()
            else:
                print("æ‘„åƒå¤´ç®¡ç†å™¨æ¨¡å—ä¸å¯ç”¨")
            input("\næŒ‰ Enter é”®ç»§ç»­...")
        
        elif sub_choice == "2":
            handle_camera_info_mode()
        
        elif sub_choice == "3":
            handle_test_camera_mode()
        
        elif sub_choice == "4":
            handle_interactive_camera_selection()
        
        elif sub_choice == "5":
            handle_find_best_camera_mode()
            input("\næŒ‰ Enter é”®ç»§ç»­...")
        
        elif sub_choice == "6":
            handle_camera_preview_mode()
        
        else:
            print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")
            input("\næŒ‰ Enter é”®ç»§ç»­...")


def handle_image_mode(args):
    """å¤„ç†å›¾ç‰‡æ¨¡å¼"""
    print("\n=== å›¾ç‰‡å¤„ç†æ¨¡å¼ ===")
    
    while True:
        image_path = input("è¯·è¾“å…¥å›¾ç‰‡è·¯å¾„ (æˆ–è¾“å…¥ 'back' è¿”å›): ").strip()
        
        if image_path.lower() == 'back':
            print("è¿”å›ä¸»èœå•")
            break
        
        if not os.path.exists(image_path):
            print(f"é”™è¯¯ï¼šå›¾ç‰‡ä¸å­˜åœ¨ {image_path}")
            print("è¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®")
            continue
        
        # è¯¢é—®å‚æ•°
        use_preprocess = input("å¯ç”¨é¢„å¤„ç†ï¼Ÿ(y/n, é»˜è®¤y): ").strip().lower()
        output_dir = input("è¾“å‡ºç›®å½• (é»˜è®¤: results): ").strip()
        
        # è®¾ç½®å‚æ•°
        args.image = image_path
        args.no_preprocess = (use_preprocess == 'n')
        args.output_dir = output_dir if output_dir else "results"
        
        # åˆ›å»ºç³»ç»Ÿ
        try:
            system = LicensePlateSystem(
                detection_model_path=args.model,
                detection_conf_threshold=args.conf,
                use_preprocessing=not args.no_preprocess
            )
            
            # å¤„ç†å›¾ç‰‡
            process_image_mode(system, args)
            
        except Exception as e:
            print(f"å¤„ç†å¤±è´¥: {e}")
        
        # è¯¢é—®æ˜¯å¦ç»§ç»­å¤„ç†å…¶ä»–å›¾ç‰‡
        another = input("\næ˜¯å¦å¤„ç†å¦ä¸€å¼ å›¾ç‰‡ï¼Ÿ(y/n): ").strip().lower()
        if another != 'y':
            break


def handle_video_mode(args):
    """å¤„ç†è§†é¢‘æ¨¡å¼"""
    print("\n=== è§†é¢‘å¤„ç†æ¨¡å¼ ===")
    
    while True:
        video_path = input("è¯·è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„ (æˆ–è¾“å…¥ 'back' è¿”å›): ").strip()
        
        if video_path.lower() == 'back':
            print("è¿”å›ä¸»èœå•")
            break
        
        if not os.path.exists(video_path):
            print(f"é”™è¯¯ï¼šè§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨ {video_path}")
            print("è¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®")
            continue
        
        # è¯¢é—®å‚æ•°
        print("\nè§†é¢‘å¤„ç†å‚æ•°:")
        start_time = input("å¼€å§‹æ—¶é—´(ç§’ï¼Œé»˜è®¤0): ").strip()
        duration = input("å¤„ç†æ—¶é•¿(ç§’ï¼Œé»˜è®¤æ•´ä¸ªè§†é¢‘): ").strip()
        detection_interval = input("æ£€æµ‹é—´éš”å¸§æ•°(é»˜è®¤10): ").strip()
        output_dir = input("è¾“å‡ºç›®å½• (é»˜è®¤: results/video): ").strip()
        
        # è®¾ç½®å‚æ•°
        args.video = video_path
        if start_time:
            args.video_start = float(start_time)
        if duration:
            args.video_duration = float(duration)
        if detection_interval:
            args.detection_interval = int(detection_interval)
        args.output_dir = output_dir if output_dir else "results/video"
        
        # åˆ›å»ºç³»ç»Ÿ
        try:
            system = LicensePlateSystem(
                detection_model_path=args.model,
                detection_conf_threshold=args.conf,
                use_preprocessing=not args.no_preprocess
            )
            
            # å¤„ç†è§†é¢‘
            process_video_mode(system, args)
            
        except Exception as e:
            print(f"å¤„ç†å¤±è´¥: {e}")
        
        # è¯¢é—®æ˜¯å¦ç»§ç»­å¤„ç†å…¶ä»–è§†é¢‘
        another = input("\næ˜¯å¦å¤„ç†å¦ä¸€ä¸ªè§†é¢‘ï¼Ÿ(y/n): ").strip().lower()
        if another != 'y':
            break


def handle_camera_mode(args):
    """å¤„ç†æ‘„åƒå¤´æ¨¡å¼ï¼ˆå®æ—¶è½¦ç‰Œæ£€æµ‹ï¼‰"""
    print("\n=== æ‘„åƒå¤´å®æ—¶æ£€æµ‹æ¨¡å¼ ===")
    
    # äº¤äº’å¼é€‰æ‹©æ‘„åƒå¤´
    if CAMERA_MANAGER_AVAILABLE:
        use_interactive = input("äº¤äº’å¼é€‰æ‹©æ‘„åƒå¤´ï¼Ÿ(y/n): ").strip().lower()
        if use_interactive == 'y':
            camera_info = select_camera_interactive()
            if camera_info is None:
                print("æ‘„åƒå¤´é€‰æ‹©å–æ¶ˆ")
                return
            args.camera_index = camera_info['index']
            if 'width' in camera_info:
                args.frame_width = camera_info['width']
            if 'height' in camera_info:
                args.frame_height = camera_info['height']
            if 'fps' in camera_info:
                args.fps = int(camera_info['fps'])
            if 'interval' in camera_info:
                args.detection_interval = camera_info['interval']
        else:
            # æ‰‹åŠ¨è¾“å…¥å‚æ•°
            camera_idx = input("æ‘„åƒå¤´ç´¢å¼•(é»˜è®¤0): ").strip()
            args.camera_index = int(camera_idx) if camera_idx else 0
    else:
        print("æ‘„åƒå¤´ç®¡ç†å™¨æ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
    
    # è¯¢é—®å…¶ä»–å‚æ•°
    print("\næ‘„åƒå¤´å‚æ•°è®¾ç½®:")
    width = input("å¸§å®½åº¦(é»˜è®¤1280): ").strip()
    height = input("å¸§é«˜åº¦(é»˜è®¤720): ").strip()
    fps = input("å¸§ç‡(é»˜è®¤30): ").strip()
    interval = input("æ£€æµ‹é—´éš”å¸§æ•°(é»˜è®¤10): ").strip()
    output_dir = input("è¾“å‡ºç›®å½• (é»˜è®¤: results/camera): ").strip()
    
    # è®¾ç½®å‚æ•°
    if width:
        args.frame_width = int(width)
    if height:
        args.frame_height = int(height)
    if fps:
        args.fps = int(fps)
    if interval:
        args.detection_interval = int(interval)
    args.output_dir = output_dir if output_dir else "results/camera"
    args.camera = True
    
    # åˆ›å»ºç³»ç»Ÿ
    try:
        system = LicensePlateSystem(
            detection_model_path=args.model,
            detection_conf_threshold=args.conf,
            use_preprocessing=not args.no_preprocess
        )
        
        # å¤„ç†æ‘„åƒå¤´
        process_camera_mode(system, args)
        
    except Exception as e:
        print(f"å¤„ç†å¤±è´¥: {e}")


def handle_batch_mode(args):
    """å¤„ç†æ‰¹é‡æ¨¡å¼"""
    print("\n=== æ‰¹é‡å¤„ç†æ¨¡å¼ ===")
    
    while True:
        batch_dir = input("è¯·è¾“å…¥å›¾ç‰‡ç›®å½•è·¯å¾„ (æˆ–è¾“å…¥ 'back' è¿”å›): ").strip()
        
        if batch_dir.lower() == 'back':
            print("è¿”å›ä¸»èœå•")
            break
        
        if not os.path.exists(batch_dir):
            print(f"é”™è¯¯ï¼šç›®å½•ä¸å­˜åœ¨ {batch_dir}")
            print("è¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®")
            continue
        
        # è¯¢é—®å‚æ•°
        output_dir = input("è¾“å‡ºç›®å½• (é»˜è®¤: results/batch): ").strip()
        
        # è®¾ç½®å‚æ•°
        args.batch = batch_dir
        args.output_dir = output_dir if output_dir else "results/batch"
        
        # åˆ›å»ºç³»ç»Ÿ
        try:
            system = LicensePlateSystem(
                detection_model_path=args.model,
                detection_conf_threshold=args.conf,
                use_preprocessing=not args.no_preprocess
            )
            
            # æ‰¹é‡å¤„ç†
            process_batch_mode(system, args)
            
        except Exception as e:
            print(f"å¤„ç†å¤±è´¥: {e}")
        
        # è¯¢é—®æ˜¯å¦ç»§ç»­å¤„ç†å…¶ä»–ç›®å½•
        another = input("\næ˜¯å¦å¤„ç†å¦ä¸€ä¸ªç›®å½•ï¼Ÿ(y/n): ").strip().lower()
        if another != 'y':
            break


def handle_camera_info_mode():
    """å¤„ç†æ‘„åƒå¤´ä¿¡æ¯æŸ¥è¯¢æ¨¡å¼"""
    print("\n=== æ‘„åƒå¤´ä¿¡æ¯æŸ¥è¯¢ ===")
    
    if not CAMERA_MANAGER_AVAILABLE:
        print("æ‘„åƒå¤´ç®¡ç†å™¨æ¨¡å—ä¸å¯ç”¨")
        return
    
    while True:
        camera_idx = input("è¯·è¾“å…¥æ‘„åƒå¤´ç´¢å¼• (æˆ–è¾“å…¥ 'list' åˆ—å‡ºæ‰€æœ‰, 'back' è¿”å›): ").strip().lower()
        
        if camera_idx == 'back':
            print("è¿”å›ä¸Šä¸€çº§")
            break
        
        elif camera_idx == 'list':
            list_all_cameras()
            continue
        
        try:
            idx = int(camera_idx)
            info = get_camera_info(idx)
            print_camera_info(info)
            
            # è¯¢é—®æ˜¯å¦æµ‹è¯•æ­¤æ‘„åƒå¤´
            test = input("\næ˜¯å¦æµ‹è¯•æ­¤æ‘„åƒå¤´ï¼Ÿ(y/n): ").strip().lower()
            if test == 'y':
                if CAMERA_MANAGER_AVAILABLE:
                    from camera_manager import test_camera
                    test_camera(idx)
        except ValueError:
            print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—ç´¢å¼•")


def handle_test_camera_mode():
    """å¤„ç†æ‘„åƒå¤´æµ‹è¯•æ¨¡å¼"""
    print("\n=== æ‘„åƒå¤´æµ‹è¯• ===")
    
    if not CAMERA_MANAGER_AVAILABLE:
        print("æ‘„åƒå¤´ç®¡ç†å™¨æ¨¡å—ä¸å¯ç”¨")
        return
    
    while True:
        camera_idx = input("è¯·è¾“å…¥æ‘„åƒå¤´ç´¢å¼• (æˆ–è¾“å…¥ 'back' è¿”å›): ").strip().lower()
        
        if camera_idx == 'back':
            print("è¿”å›ä¸Šä¸€çº§")
            break
        
        try:
            idx = int(camera_idx)
            from camera_manager import test_camera
            test_camera(idx)
        except ValueError:
            print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—ç´¢å¼•")


def handle_interactive_camera_selection():
    """å¤„ç†äº¤äº’å¼æ‘„åƒå¤´é€‰æ‹©"""
    print("\n=== äº¤äº’å¼æ‘„åƒå¤´é€‰æ‹© ===")
    
    if not CAMERA_MANAGER_AVAILABLE:
        print("æ‘„åƒå¤´ç®¡ç†å™¨æ¨¡å—ä¸å¯ç”¨")
        return
    
    camera_info = select_camera_interactive()
    if camera_info is not None:
        print(f"\nå·²é€‰æ‹©æ‘„åƒå¤´:")
        print(f"  ç´¢å¼•: {camera_info['index']}")
        print(f"  åç§°: {camera_info.get('name', 'æœªçŸ¥')}")
        if 'width' in camera_info and 'height' in camera_info:
            print(f"  åˆ†è¾¨ç‡: {camera_info['width']}x{camera_info['height']}")
        if 'fps' in camera_info:
            print(f"  å¸§ç‡: {camera_info['fps']:.1f}fps")
        
        # è¯¢é—®æ˜¯å¦ç«‹å³å¼€å§‹è½¦ç‰Œæ£€æµ‹
        start_detection = input("\næ˜¯å¦ç«‹å³å¼€å§‹è½¦ç‰Œæ£€æµ‹ï¼Ÿ(y/n): ").strip().lower()
        if start_detection == 'y':
            # åˆ›å»ºä¸´æ—¶å‚æ•°å¯¹è±¡
            class Args:
                pass
            args = Args()
            args.camera_index = camera_info['index']
            args.frame_width = camera_info.get('width', 1280)
            args.frame_height = camera_info.get('height', 720)
            args.fps = int(camera_info.get('fps', 30))
            args.detection_interval = camera_info.get('interval', 10)
            args.output_dir = "results/camera"
            args.model = "yolov8s.pt"
            args.conf = 0.5
            args.no_preprocess = False
            
            # åˆ›å»ºç³»ç»Ÿå¹¶å¼€å§‹æ£€æµ‹
            try:
                system = LicensePlateSystem(
                    detection_model_path=args.model,
                    detection_conf_threshold=args.conf,
                    use_preprocessing=not args.no_preprocess
                )
                process_camera_mode(system, args)
            except Exception as e:
                print(f"å¯åŠ¨è½¦ç‰Œæ£€æµ‹å¤±è´¥: {e}")


def handle_find_best_camera_mode():
    """å¤„ç†å¯»æ‰¾æœ€ä½³æ‘„åƒå¤´æ¨¡å¼"""
    print("\n=== å¯»æ‰¾æœ€ä½³æ‘„åƒå¤´ ===")
    
    if not CAMERA_MANAGER_AVAILABLE:
        print("æ‘„åƒå¤´ç®¡ç†å™¨æ¨¡å—ä¸å¯ç”¨")
        return
    
    best_camera = find_best_camera()
    if best_camera:
        print(f"æœ€ä½³æ‘„åƒå¤´: ç´¢å¼• {best_camera['index']}")
        print(f"åç§°: {best_camera.get('name', 'æœªçŸ¥')}")
        print(f"åˆ†è¾¨ç‡: {best_camera['width']}x{best_camera['height']}")
        print(f"å¸§ç‡: {best_camera['fps']:.1f}fps")
        
        # è¯¢é—®æ˜¯å¦æµ‹è¯•æ­¤æ‘„åƒå¤´
        test = input("\næ˜¯å¦æµ‹è¯•æ­¤æ‘„åƒå¤´ï¼Ÿ(y/n): ").strip().lower()
        if test == 'y':
            from camera_manager import test_camera
            test_camera(best_camera['index'])
    else:
        print("æœªæ‰¾åˆ°æ‘„åƒå¤´")


def handle_camera_preview_mode():
    """æ‘„åƒå¤´å®æ—¶é¢„è§ˆæ¨¡å¼"""
    print("\n=== æ‘„åƒå¤´å®æ—¶é¢„è§ˆ ===")
    
    if not CAMERA_MANAGER_AVAILABLE:
        print("æ‘„åƒå¤´ç®¡ç†å™¨æ¨¡å—ä¸å¯ç”¨")
        # å°è¯•ä½¿ç”¨é»˜è®¤æ–¹æ³•
        camera_idx = input("è¯·è¾“å…¥æ‘„åƒå¤´ç´¢å¼• (é»˜è®¤0): ").strip()
        camera_idx = int(camera_idx) if camera_idx else 0
        test_single_camera(camera_idx)
        return
    
    # å…ˆåˆ—å‡ºæ‰€æœ‰æ‘„åƒå¤´
    list_all_cameras()
    
    # é€‰æ‹©æ‘„åƒå¤´
    camera_idx = input("\nè¯·è¾“å…¥è¦é¢„è§ˆçš„æ‘„åƒå¤´ç´¢å¼• (æˆ–è¾“å…¥ 'back' è¿”å›): ").strip().lower()
    
    if camera_idx == 'back':
        print("è¿”å›ä¸Šä¸€çº§")
        return
    
    try:
        idx = int(camera_idx)
        from camera_manager import test_camera
        test_camera(idx)
    except ValueError:
        print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—ç´¢å¼•")


def print_help_info():
    """æ‰“å°å¸®åŠ©ä¿¡æ¯"""
    print("\n" + "=" * 60)
    print("å¸®åŠ©ä¿¡æ¯")
    print("=" * 60)
    print("ç³»ç»ŸåŠŸèƒ½:")
    print("  1. å›¾ç‰‡å¤„ç† - è¯†åˆ«å•å¼ å›¾ç‰‡ä¸­çš„è½¦ç‰Œ")
    print("  2. è§†é¢‘å¤„ç† - è¯†åˆ«è§†é¢‘æ–‡ä»¶ä¸­çš„è½¦ç‰Œ")
    print("  3. å®æ—¶æ£€æµ‹ - é€šè¿‡æ‘„åƒå¤´å®æ—¶æ£€æµ‹è½¦ç‰Œ")
    print("  4. æ‰¹é‡å¤„ç† - å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰å›¾ç‰‡")
    print("  5. æ‘„åƒå¤´ç®¡ç† - æŸ¥çœ‹å’Œç®¡ç†æ‘„åƒå¤´è®¾å¤‡")
    print("  6. ç³»ç»Ÿæµ‹è¯• - è¿è¡Œç³»ç»Ÿè¯Šæ–­å’Œæµ‹è¯•")
    print()
    print("æ‘„åƒå¤´ç®¡ç†åŠŸèƒ½:")
    print("  1. åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ‘„åƒå¤´")
    print("  2. æŸ¥çœ‹æ‘„åƒå¤´è¯¦ç»†ä¿¡æ¯")
    print("  3. æµ‹è¯•æ‘„åƒå¤´")
    print("  4. äº¤äº’å¼é€‰æ‹©æ‘„åƒå¤´")
    print("  5. å¯»æ‰¾æœ€ä½³æ‘„åƒå¤´")
    print("  6. æ‘„åƒå¤´å®æ—¶é¢„è§ˆ")
    print()
    print("å¸¸ç”¨å‘½ä»¤:")
    print("  python main.py --image test.jpg")
    print("  python main.py --video test.mp4")
    print("  python main.py --camera")
    print("  python main.py --batch images/")
    print()
    print("æ›´å¤šé€‰é¡¹ä½¿ç”¨: python main.py --help")
    print("=" * 60)


def process_image_mode(system, args):
    """å¤„ç†å›¾ç‰‡æ¨¡å¼"""
    print(f"å¤„ç†å›¾ç‰‡: {args.image}")
    
    if not os.path.exists(args.image):
        print(f"é”™è¯¯ï¼šå›¾ç‰‡ä¸å­˜åœ¨ {args.image}")
        return
    
    try:
        results = system.process_image(
            image_path=args.image,
            save_results=True,
            output_dir=args.output_dir
        )
        
        if results:
            print(f"\nâœ“ å¤„ç†å®Œæˆï¼æ£€æµ‹åˆ° {len(results)} ä¸ªè½¦ç‰Œ")
            
            # æ˜¾ç¤ºç»“æœ
            for i, result in enumerate(results, 1):
                print(f"  {i}. {result['plate_text']} ({result['plate_type']}) "
                      f"ç½®ä¿¡åº¦: {result['ocr_confidence']:.2f}")
        else:
            print("æœªæ£€æµ‹åˆ°è½¦ç‰Œ")
            
    except Exception as e:
        print(f"å¤„ç†å›¾ç‰‡æ—¶å‡ºé”™: {e}")
        if args.debug:
            import traceback
            traceback.print_exc()


def process_camera_mode(system, args):
    """å¤„ç†æ‘„åƒå¤´æ¨¡å¼"""
    print("å¯åŠ¨æ‘„åƒå¤´å®æ—¶æ£€æµ‹...")
    
    try:
        # æ£€æŸ¥æ‘„åƒå¤´å¯ç”¨æ€§
        cap = cv2.VideoCapture(args.camera_index)
        if not cap.isOpened():
            print(f"é”™è¯¯ï¼šæ— æ³•æ‰“å¼€æ‘„åƒå¤´ {args.camera_index}")
            
            # å°è¯•è‡ªåŠ¨æ£€æµ‹å¯ç”¨æ‘„åƒå¤´
            print("å°è¯•è‡ªåŠ¨æ£€æµ‹å¯ç”¨æ‘„åƒå¤´...")
            for i in range(3):
                test_cap = cv2.VideoCapture(i)
                if test_cap.isOpened():
                    print(f"æ‰¾åˆ°å¯ç”¨æ‘„åƒå¤´: ç´¢å¼• {i}")
                    args.camera_index = i
                    test_cap.release()
                    cap = cv2.VideoCapture(i)
                    break
                test_cap.release()
            
            if not cap.isOpened():
                print("æœªæ‰¾åˆ°å¯ç”¨æ‘„åƒå¤´")
                return
        
        cap.release()
        
        # ä½¿ç”¨å®æ—¶æ£€æµ‹å™¨
        print("=" * 60)
        print("æ‘„åƒå¤´å‚æ•°:")
        print(f"  ç´¢å¼•: {args.camera_index}")
        print(f"  åˆ†è¾¨ç‡: {args.frame_width}x{args.frame_height}")
        print(f"  å¸§ç‡: {args.fps}fps")
        print(f"  æ£€æµ‹é—´éš”: æ¯{args.detection_interval}å¸§æ£€æµ‹ä¸€æ¬¡")
        print("=" * 60)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        camera_output_dir = os.path.join(args.output_dir, "camera")
        os.makedirs(camera_output_dir, exist_ok=True)
        
        # å¯åŠ¨æ‘„åƒå¤´æ£€æµ‹
        system.start_camera_detection(
            camera_index=args.camera_index,
            frame_width=args.frame_width,
            frame_height=args.frame_height,
            fps=args.fps,
            detection_interval=args.detection_interval,
            output_dir=camera_output_dir
        )
        
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"æ‘„åƒå¤´æ£€æµ‹æ—¶å‡ºé”™: {e}")
        if args.debug:
            import traceback
            traceback.print_exc()


def test_single_camera(camera_index, test_duration=5):
    """æµ‹è¯•å•ä¸ªæ‘„åƒå¤´"""
    print(f"\næµ‹è¯•æ‘„åƒå¤´ {camera_index}...")
    print("æŒ‰ 'q' é”®é€€å‡ºæµ‹è¯•")
    
    cap = None
    try:
        # å°è¯•ä¸åŒçš„åç«¯
        backends_to_try = [cv2.CAP_ANY, cv2.CAP_DSHOW, cv2.CAP_MSMF]
        
        for backend in backends_to_try:
            try:
                cap = cv2.VideoCapture(camera_index, backend)
                if cap.isOpened():
                    print(f"  ä½¿ç”¨åç«¯: {backend}")
                    break
            except:
                continue
        
        if not cap or not cap.isOpened():
            print("  æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
            return
        
        # è®¾ç½®æ‘„åƒå¤´å‚æ•°
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        cap.set(cv2.CAP_PROP_FPS, 30)
        
        # è·å–å®é™…å‚æ•°
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        
        print(f"  å®é™…å‚æ•°: {width}x{height} @ {fps}fps")
        print("  æ­£åœ¨æ˜¾ç¤ºæ‘„åƒå¤´ç”»é¢...")
        
        import time
        start_time = time.time()
        frame_count = 0
        
        while time.time() - start_time < test_duration:
            ret, frame = cap.read()
            if not ret:
                print("  æ— æ³•è¯»å–å¸§")
                break
            
            frame_count += 1
            
            # æ˜¾ç¤ºå¸§
            cv2.putText(frame, f"Camera {camera_index}", (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            cv2.putText(frame, f"{width}x{height}", (10, 60),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            cv2.putText(frame, f"FPS: {fps}", (10, 90),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            
            cv2.imshow(f'Camera Test - Index {camera_index}', frame)
            
            # æŒ‰'q'é€€å‡º
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        
        elapsed_time = time.time() - start_time
        if elapsed_time > 0:
            actual_fps = frame_count / elapsed_time
            print(f"  å®é™…å¸§ç‡: {actual_fps:.1f} fps")
            print(f"  æ€»å¸§æ•°: {frame_count}")
        
        print("  æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"  æµ‹è¯•å‡ºé”™: {e}")
    finally:
        if cap:
            cap.release()
        cv2.destroyAllWindows()


def process_video_mode(system, args):
    """å¤„ç†è§†é¢‘æ¨¡å¼"""
    print(f"å¤„ç†è§†é¢‘: {args.video}")
    
    if not os.path.exists(args.video):
        print(f"é”™è¯¯ï¼šè§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨ {args.video}")
        return
    
    if not VIDEO_PROCESSOR_AVAILABLE:
        print("é”™è¯¯ï¼šè§†é¢‘å¤„ç†å™¨æ¨¡å—æœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿ video_processor.py å­˜åœ¨")
        print("æ‚¨å¯ä»¥ç»§ç»­ä½¿ç”¨å…¶ä»–åŠŸèƒ½ï¼Œæˆ–è€…ä¿®å¤å¯¼å…¥é—®é¢˜")
        response = input("æ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ").strip().lower()
        if response != 'y':
            return
    
    try:
        # åˆ›å»ºè§†é¢‘å¤„ç†å™¨
        if VIDEO_PROCESSOR_AVAILABLE:
            video_processor = VideoLicensePlateProcessor.from_system(system)
        else:
            # ä½¿ç”¨ç®€å•çš„è§†é¢‘å¤„ç†æ›¿ä»£æ–¹æ¡ˆ
            video_processor = None
            print("è­¦å‘Šï¼šä½¿ç”¨ç®€åŒ–è§†é¢‘å¤„ç†æ¨¡å¼")
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            video_output_dir = os.path.join(args.output_dir, "video")
            os.makedirs(video_output_dir, exist_ok=True)
            
            # æ‰“å¼€è§†é¢‘æ–‡ä»¶
            cap = cv2.VideoCapture(args.video)
            if not cap.isOpened():
                print(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {args.video}")
                return
            
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = int(cap.get(cv2.CAP_PROP_FPS))
            
            print(f"è§†é¢‘ä¿¡æ¯: {total_frames}å¸§, {fps}fps")
            
            # å¤„ç†è§†é¢‘
            frame_idx = 0
            detection_count = 0
            unique_plates = set()
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_idx += 1
                
                # æ¯30å¸§å¤„ç†ä¸€æ¬¡ï¼ˆæˆ–è‡ªå®šä¹‰é—´éš”ï¼‰
                if frame_idx % (args.detection_interval if hasattr(args, 'detection_interval') else 30) == 0:
                    # ä¿å­˜ä¸´æ—¶å¸§
                    temp_path = f"temp_frame_{frame_idx}.jpg"
                    cv2.imwrite(temp_path, frame)
                    
                    # æ£€æµ‹è½¦ç‰Œ
                    plates_info = system.detector.detect_all_and_rectify(temp_path)
                    
                    if plates_info:
                        for plate_info in plates_info:
                            detection_count += 1
                            
                            # å¤„ç†è½¦ç‰Œ
                            result = system._process_camera_detection(frame, plate_info, detection_count)
                            
                            if result.get('plate_text') != "æœªçŸ¥":
                                unique_plates.add(result['plate_text'])
                                
                                # ä¿å­˜ç»“æœå¸§
                                if not hasattr(args, 'no_save') or not args.no_save:
                                    result_frame = system._annotate_camera_frame(frame.copy(), result)
                                    cv2.imwrite(f"{video_output_dir}/frame_{frame_idx}_plate_{detection_count}.jpg", result_frame)
                    
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    try:
                        os.remove(temp_path)
                    except:
                        pass
                
                # æ˜¾ç¤ºè¿›åº¦
                if frame_idx % 100 == 0:
                    print(f"å·²å¤„ç† {frame_idx}/{total_frames} å¸§")
            
            cap.release()
            
            print(f"\nè§†é¢‘å¤„ç†å®Œæˆ:")
            print(f"  æ€»å¸§æ•°: {total_frames}")
            print(f"  æ£€æµ‹åˆ°è½¦ç‰Œæ•°: {detection_count}")
            print(f"  å”¯ä¸€è½¦ç‰Œæ•°: {len(unique_plates)}")
            print(f"  è¾“å‡ºç›®å½•: {video_output_dir}")
            
            # ä¿å­˜æ±‡æ€»ç»“æœ
            with open(f"{video_output_dir}/summary.txt", "w", encoding="utf-8") as f:
                f.write(f"è§†é¢‘æ–‡ä»¶: {args.video}\n")
                f.write(f"æ€»å¸§æ•°: {total_frames}\n")
                f.write(f"æ£€æµ‹åˆ°è½¦ç‰Œæ•°: {detection_count}\n")
                f.write(f"å”¯ä¸€è½¦ç‰Œæ•°: {len(unique_plates)}\n")
                f.write(f"æ£€æµ‹åˆ°çš„è½¦ç‰Œ: {', '.join(unique_plates)}\n")
            
            return
        
        # ä½¿ç”¨è§†é¢‘å¤„ç†å™¨
        result = video_processor.process_video_file(
            video_path=args.video,
            output_dir=args.output_dir,
            detection_interval=args.detection_interval if hasattr(args, 'detection_interval') else 10,
            save_results=not args.no_save if hasattr(args, 'no_save') else True,
            save_frames=args.save_all if hasattr(args, 'save_all') else False,
            save_json=args.save_json if hasattr(args, 'save_json') else True,
            display=not args.no_display if hasattr(args, 'no_display') else True,
            start_time=args.video_start,
            duration=args.video_duration,
            output_fps=args.video_output_fps,
            show_progress=True,
            max_frames=args.max_frames
        )
        
        if result.get('success', False):
            print(f"\nâœ“ è§†é¢‘å¤„ç†å®Œæˆï¼")
            print(f"  æ€»å¸§æ•°: {result['total_frames']}")
            print(f"  å¤„ç†å¸§æ•°: {result['processed_frames']}")
            print(f"  æ£€æµ‹åˆ°è½¦ç‰Œæ•°: {result['detection_count']}")
            print(f"  å”¯ä¸€è½¦ç‰Œæ•°: {result['unique_plates']}")
            print(f"  è¾“å‡ºç›®å½•: {result['output_dir']}")
            
            # æ˜¾ç¤ºæ£€æµ‹åˆ°çš„è½¦ç‰Œ
            if result['detections']:
                print(f"\næ£€æµ‹åˆ°çš„è½¦ç‰Œ:")
                for detection in result['detections'][:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                    plate_text = detection.get('plate_text', 'æœªçŸ¥')
                    plate_type = detection.get('plate_type', 'æœªçŸ¥')
                    frame_idx = detection.get('frame_index', 0)
                    conf = detection.get('ocr_confidence', 0)
                    print(f"  ç¬¬{frame_idx}å¸§: {plate_text} ({plate_type}) ç½®ä¿¡åº¦: {conf:.2f}")
                
                if len(result['detections']) > 10:
                    print(f"  ... è¿˜æœ‰{len(result['detections']) - 10}ä¸ªæ£€æµ‹ç»“æœ")
        else:
            print(f"\nâœ— è§†é¢‘å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
    except Exception as e:
        print(f"å¤„ç†è§†é¢‘æ—¶å‡ºé”™: {e}")
        if args.debug:
            import traceback
            traceback.print_exc()


def process_batch_mode(system, args):
    """æ‰¹é‡å¤„ç†æ¨¡å¼"""
    print(f"æ‰¹é‡å¤„ç†ç›®å½•: {args.batch}")
    
    if not os.path.exists(args.batch):
        print(f"é”™è¯¯ï¼šç›®å½•ä¸å­˜åœ¨ {args.batch}")
        return
    
    # æ”¶é›†æ‰€æœ‰å›¾ç‰‡
    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']
    image_files = []
    
    for ext in image_extensions:
        image_files.extend(Path(args.batch).glob(f"*{ext}"))
        image_files.extend(Path(args.batch).glob(f"*{ext.upper()}"))
    
    if not image_files:
        print("æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")
        return
    
    print(f"æ‰¾åˆ° {len(image_files)} å¼ å›¾ç‰‡")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    batch_output_dir = os.path.join(args.output_dir, "batch")
    os.makedirs(batch_output_dir, exist_ok=True)
    
    # æ‰¹é‡å¤„ç†
    total_plates = 0
    success_count = 0
    
    for i, image_file in enumerate(image_files, 1):
        print(f"\n[{i}/{len(image_files)}] å¤„ç†: {image_file.name}")
        
        try:
            # ä¸ºæ¯å¼ å›¾ç‰‡åˆ›å»ºå­ç›®å½•
            image_output_dir = os.path.join(batch_output_dir, image_file.stem)
            os.makedirs(image_output_dir, exist_ok=True)
            
            # å¤„ç†å›¾ç‰‡
            results = system.process_image(
                image_path=str(image_file),
                save_results=True,
                output_dir=image_output_dir
            )
            
            if results:
                success_count += 1
                total_plates += len(results)
                print(f"  âœ“ æ£€æµ‹åˆ° {len(results)} ä¸ªè½¦ç‰Œ")
            else:
                print(f"  âœ— æœªæ£€æµ‹åˆ°è½¦ç‰Œ")
                
        except Exception as e:
            print(f"  å¤„ç†å¤±è´¥: {e}")
    
    # æ‰“å°æ±‡æ€»
    print("\n" + "=" * 60)
    print("æ‰¹é‡å¤„ç†å®Œæˆ")
    print("=" * 60)
    print(f"å¤„ç†å›¾ç‰‡æ•°: {len(image_files)}")
    print(f"æˆåŠŸæ£€æµ‹å›¾ç‰‡æ•°: {success_count}")
    print(f"æ£€æµ‹åˆ°è½¦ç‰Œæ€»æ•°: {total_plates}")
    print(f"è¾“å‡ºç›®å½•: {batch_output_dir}")


def run_tests():
    """è¿è¡Œç³»ç»Ÿæµ‹è¯•"""
    print("è¿è¡Œç³»ç»Ÿæµ‹è¯•...")
    
    # æ£€æŸ¥ä¾èµ–
    print("\n1. æ£€æŸ¥ä¾èµ–åº“:")
    try:
        import torch
        print(f"  âœ“ PyTorch: {torch.__version__}")
        if torch.cuda.is_available():
            device_count = torch.cuda.device_count()
            for i in range(device_count):
                print(f"    GPU {i}: {torch.cuda.get_device_name(i)}")
        else:
            print(f"    è®¾å¤‡: CPU")
    except:
        print("  âœ— PyTorch: æœªå®‰è£…")
    
    try:
        print(f"  âœ“ OpenCV: {cv2.__version__}")
    except:
        print("  âœ— OpenCV: æœªå®‰è£…")
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    print("\n2. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶:")
    model_files = ["yolov8s.pt", "yolov8n.pt", "yolov8m.pt", "yolov8l.pt", "yolov8x.pt"]
    found_models = []
    for model_file in model_files:
        if os.path.exists(model_file):
            file_size = os.path.getsize(model_file) / (1024*1024)  # è½¬æ¢ä¸ºMB
            print(f"  âœ“ {model_file}: å­˜åœ¨ ({file_size:.1f} MB)")
            found_models.append(model_file)
        else:
            print(f"  âœ— {model_file}: ä¸å­˜åœ¨")
    
    if found_models:
        print(f"    æ‰¾åˆ° {len(found_models)} ä¸ªæ¨¡å‹æ–‡ä»¶")
    else:
        print("    è­¦å‘Š: æœªæ‰¾åˆ°ä»»ä½•æ¨¡å‹æ–‡ä»¶")
        print("    è¯·ä»ä»¥ä¸‹é“¾æ¥ä¸‹è½½:")
        print("    https://github.com/ultralytics/ultralytics")
    
    # æ£€æŸ¥æ‘„åƒå¤´ - æ”¹è¿›ç‰ˆæœ¬
    print("\n3. æ£€æŸ¥æ‘„åƒå¤´:")
    available_cameras = []
    max_check_index = 10  # æœ€å¤šæ£€æŸ¥10ä¸ªæ‘„åƒå¤´ç´¢å¼•
    
    # å®šä¹‰è¦å°è¯•çš„åç«¯åˆ—è¡¨
    backends = [
        cv2.CAP_DSHOW,    # DirectShow (Windows)
        cv2.CAP_MSMF,     # Microsoft Media Foundation (Windows)
        cv2.CAP_V4L2,     # Video4Linux (Linux)
        cv2.CAP_ANY,      # è‡ªåŠ¨é€‰æ‹©
    ]
    
    backend_names = {
        cv2.CAP_DSHOW: "DSHOW",
        cv2.CAP_MSMF: "MSMF",
        cv2.CAP_V4L2: "V4L2",
        cv2.CAP_ANY: "AUTO"
    }
    
    print("  æ­£åœ¨æ‰«ææ‘„åƒå¤´...")
    
    for i in range(max_check_index):
        camera_found = False
        best_backend = None
        camera_info = None
        
        # å°è¯•ä¸åŒçš„åç«¯
        for backend in backends:
            try:
                # ä½¿ç”¨try-excepté¿å…å´©æºƒ
                cap = cv2.VideoCapture(i, backend)
                if cap.isOpened():
                    # å°è¯•è¯»å–ä¸€å¸§
                    ret, frame = cap.read()
                    if ret and frame is not None:
                        # æˆåŠŸè¯»å–åˆ°å¸§
                        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) or 0)
                        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) or 0)
                        fps = int(cap.get(cv2.CAP_PROP_FPS) or 0)
                        
                        # å°è¯•å¤šæ¬¡è¯»å–ä»¥è·å–æ›´å‡†ç¡®çš„fps
                        if fps == 0:
                            import time
                            start_time = time.time()
                            frame_count = 0
                            for _ in range(30):  # è¯»å–30å¸§è®¡ç®—fps
                                ret, _ = cap.read()
                                if ret:
                                    frame_count += 1
                            end_time = time.time()
                            if end_time - start_time > 0:
                                fps = int(frame_count / (end_time - start_time))
                        
                        camera_info = {
                            'index': i,
                            'backend': backend,
                            'backend_name': backend_names.get(backend, str(backend)),
                            'width': width if width > 0 else "æœªçŸ¥",
                            'height': height if height > 0 else "æœªçŸ¥",
                            'fps': fps if fps > 0 else "æœªçŸ¥",
                            'working': True
                        }
                        
                        best_backend = backend
                        camera_found = True
                        
                        # é‡Šæ”¾æ‘„åƒå¤´
                        cap.release()
                        break  # æ‰¾åˆ°å¯ç”¨åç«¯å°±åœæ­¢å°è¯•
                    else:
                        cap.release()
            except Exception as e:
                # å¿½ç•¥ç‰¹å®šåç«¯é”™è¯¯ï¼Œç»§ç»­å°è¯•å…¶ä»–åç«¯
                pass
        
        if camera_found and camera_info:
            # è·å–æ‘„åƒå¤´åç§°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            camera_name = f"æ‘„åƒå¤´ {i}"
            try:
                # å°è¯•ä½¿ç”¨æœ€åä¸€ä¸ªæˆåŠŸçš„åç«¯å†æ¬¡æ‰“å¼€ä»¥è·å–åç§°
                if best_backend:
                    cap = cv2.VideoCapture(i, best_backend)
                    if cap.isOpened():
                        # åœ¨æŸäº›ç³»ç»Ÿä¸Šå¯ä»¥è·å–è®¾å¤‡åç§°
                        try:
                            # å°è¯•é€šè¿‡å±æ€§è·å–åç§°
                            name = cap.get(cv2.CAP_PROP_BACKEND_NAME)
                            if name:
                                camera_name = f"æ‘„åƒå¤´ {i} ({name})"
                        except:
                            pass
                        cap.release()
            except:
                pass
            
            print(f"  âœ“ {camera_name} ({camera_info['backend_name']}) - "
                  f"{camera_info['width']}x{camera_info['height']} @ {camera_info['fps']}fps: å¯ç”¨")
            
            available_cameras.append({
                'index': i,
                'name': camera_name,
                'backend': camera_info['backend_name'],
                'width': camera_info['width'],
                'height': camera_info['height'],
                'fps': camera_info['fps']
            })
        else:
            # å¦‚æœå‰å‡ ä¸ªéƒ½æ²¡æœ‰æ‘„åƒå¤´ï¼Œæå‰ç»“æŸ
            if i > 3 and len(available_cameras) == 0:
                # æ£€æŸ¥å‰4ä¸ªéƒ½æ²¡æœ‰æ‘„åƒå¤´ï¼Œå°±ä¸å†ç»§ç»­æ£€æŸ¥å¤ªå¤š
                if i >= 5:
                    print(f"  æ‰«æåˆ°ç´¢å¼• {i}ï¼Œæœªå‘ç°æ›´å¤šæ‘„åƒå¤´")
                    break
    
    if available_cameras:
        print(f"\n    æ‰¾åˆ° {len(available_cameras)} ä¸ªå¯ç”¨æ‘„åƒå¤´")
        print("\n    å¯ç”¨æ‘„åƒå¤´åˆ—è¡¨:")
        for cam in available_cameras:
            print(f"      ç´¢å¼• {cam['index']}: {cam['name']}")
            print(f"        åç«¯: {cam['backend']}")
            print(f"        åˆ†è¾¨ç‡: {cam['width']}x{cam['height']}")
            print(f"        å¸§ç‡: {cam['fps']}fps")
        
        # æä¾›ä¸€ä¸ªç®€å•çš„æ‘„åƒå¤´æµ‹è¯•åŠŸèƒ½
        print("\n    å¿«é€Ÿæ‘„åƒå¤´æµ‹è¯•ï¼ˆè¾“å…¥ 'q' é€€å‡ºï¼‰:")
        if len(available_cameras) > 0:
            test_cam = input(f"    æ˜¯å¦æµ‹è¯•æ‘„åƒå¤´ {available_cameras[0]['index']}? (y/n): ").strip().lower()
            if test_cam == 'y':
                test_single_camera(available_cameras[0]['index'])
    else:
        print("    æœªæ‰¾åˆ°å¯ç”¨æ‘„åƒå¤´")
        print("\n    å¯èƒ½çš„åŸå› :")
        print("      1. æ‘„åƒå¤´æœªè¿æ¥æˆ–æœªæ­£ç¡®å®‰è£…")
        print("      2. æ‘„åƒå¤´é©±åŠ¨ç¨‹åºæœªæ­£ç¡®å®‰è£…")
        print("      3. æ‘„åƒå¤´è¢«å…¶ä»–ç¨‹åºå ç”¨")
        print("      4. æƒé™é—®é¢˜ï¼ˆLinux/Macéœ€è¦æƒé™ï¼‰")
        print("\n    è§£å†³æ–¹æ³•:")
        print("      1. æ£€æŸ¥æ‘„åƒå¤´ç‰©ç†è¿æ¥")
        print("      2. é‡å¯ç”µè„‘")
        print("      3. æ›´æ–°æ‘„åƒå¤´é©±åŠ¨ç¨‹åº")
        print("      4. å…³é—­å…¶ä»–ä½¿ç”¨æ‘„åƒå¤´çš„ç¨‹åº")
        print("      5. åœ¨Linux/Macä¸Šå°è¯•: sudo chmod 666 /dev/video*")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è™šæ‹Ÿæ‘„åƒå¤´è½¯ä»¶
        print("\n    è™šæ‹Ÿæ‘„åƒå¤´é€‰é¡¹:")
        print("      1. OBS Studio (å¯ä»¥åˆ›å»ºè™šæ‹Ÿæ‘„åƒå¤´)")
        print("      2. ManyCam")
        print("      3. CamTwist (Mac)")
    
    # æ£€æŸ¥æµ‹è¯•å›¾ç‰‡
    print("\n4. æ£€æŸ¥æµ‹è¯•å›¾ç‰‡:")
    test_images = ["test.jpg", "test1.jpg", "test2.jpg", "car.jpg", "license_plate.jpg", "test_plate.jpg"]
    found_images = []
    
    for test_image in test_images:
        if os.path.exists(test_image):
            file_size = os.path.getsize(test_image) / 1024  # è½¬æ¢ä¸ºKB
            print(f"  âœ“ {test_image}: å­˜åœ¨ ({file_size:.1f} KB)")
            found_images.append(test_image)
        else:
            print(f"  âœ— {test_image}: ä¸å­˜åœ¨")
    
    if found_images:
        print(f"    æ‰¾åˆ° {len(found_images)} ä¸ªæµ‹è¯•å›¾ç‰‡")
        
        # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹å›¾ç‰‡ä¿¡æ¯
        if len(found_images) > 0:
            print("\n    ç¤ºä¾‹å›¾ç‰‡é¢„è§ˆ:")
            for img_file in found_images[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                try:
                    img = cv2.imread(img_file)
                    if img is not None:
                        h, w = img.shape[:2]
                        print(f"      {img_file}: {w}x{h} åƒç´ ")
                except:
                    pass
    else:
        print("    è­¦å‘Š: æœªæ‰¾åˆ°ä»»ä½•æµ‹è¯•å›¾ç‰‡")
        print("    å»ºè®®åˆ›å»ºä»¥ä¸‹æµ‹è¯•æ–‡ä»¶:")
        print("      test.jpg - ç”¨äºæµ‹è¯•")
        print("      test_plate.jpg - è½¦ç‰Œæµ‹è¯•")
    
    # æ£€æŸ¥æµ‹è¯•è§†é¢‘
    print("\n5. æ£€æŸ¥æµ‹è¯•è§†é¢‘:")
    test_videos = ["test_video.mp4", "test_video.avi", "test_video.mov", "car_video.mp4", "test.mp4", "sample.mp4"]
    found_videos = []
    
    for test_video in test_videos:
        if os.path.exists(test_video):
            file_size = os.path.getsize(test_video) / (1024*1024)  # è½¬æ¢ä¸ºMB
            # å°è¯•æ‰“å¼€è§†é¢‘
            cap = cv2.VideoCapture(test_video)
            if cap.isOpened():
                frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                fps = int(cap.get(cv2.CAP_PROP_FPS))
                if frames > 0 and fps > 0:
                    duration = frames / fps
                    print(f"  âœ“ {test_video}: å¯ç”¨ ({frames}å¸§, {fps}fps, {duration:.1f}ç§’, {file_size:.1f} MB)")
                    found_videos.append(test_video)
                else:
                    print(f"  ? {test_video}: å¯ä»¥æ‰“å¼€ä½†æ— æ³•è·å–ä¿¡æ¯")
                cap.release()
            else:
                print(f"  âœ— {test_video}: å­˜åœ¨ä½†æ— æ³•æ‰“å¼€")
        else:
            print(f"  âœ— {test_video}: ä¸å­˜åœ¨")
    
    if found_videos:
        print(f"    æ‰¾åˆ° {len(found_videos)} ä¸ªæµ‹è¯•è§†é¢‘")
    else:
        print("    è­¦å‘Š: æœªæ‰¾åˆ°ä»»ä½•æµ‹è¯•è§†é¢‘")
        print("    å¯ä»¥å½•åˆ¶æˆ–ä¸‹è½½ä¸€äº›æµ‹è¯•è§†é¢‘")
    
    # æ£€æŸ¥ç³»ç»Ÿé…ç½®
    print("\n6. ç³»ç»Ÿé…ç½®:")
    
    # Pythonç‰ˆæœ¬
    print(f"  Pythonç‰ˆæœ¬: {sys.version.split()[0]}")
    
    # æ“ä½œç³»ç»Ÿ
    import platform
    print(f"  æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}")
    
    # å†…å­˜ä¿¡æ¯
    try:
        import psutil
        memory = psutil.virtual_memory()
        print(f"  å†…å­˜: {memory.total // (1024**3)} GB (å¯ç”¨: {memory.available // (1024**3)} GB)")
    except:
        print("  å†…å­˜ä¿¡æ¯: éœ€è¦å®‰è£…psutilåº“")
    
    # ç£ç›˜ç©ºé—´
    try:
        disk = psutil.disk_usage('.')
        print(f"  ç£ç›˜ç©ºé—´: {disk.free // (1024**3)} GB å¯ç”¨ / {disk.total // (1024**3)} GB æ€»é‡")
    except:
        pass
    
    # æ€§èƒ½æµ‹è¯•
    print("\n7. æ€§èƒ½æµ‹è¯•:")
    
    # OpenCV åŸºæœ¬æ“ä½œæµ‹è¯•
    try:
        import time
        # åˆ›å»ºä¸€ä¸ªæµ‹è¯•å›¾åƒ
        test_img = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
        
        # æµ‹è¯•å›¾åƒå¤„ç†é€Ÿåº¦
        start_time = time.time()
        for _ in range(10):
            gray = cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)
            blurred = cv2.GaussianBlur(gray, (5, 5), 0)
        cv2_time = (time.time() - start_time) / 10
        print(f"  OpenCVå¤„ç†é€Ÿåº¦: {cv2_time*1000:.1f} ms/å›¾åƒ")
    except:
        print("  OpenCVæµ‹è¯•å¤±è´¥")
    
    # NumPy æµ‹è¯•
    try:
        start_time = time.time()
        for _ in range(100):
            a = np.random.rand(1000, 1000)
            b = np.random.rand(1000, 1000)
            c = np.dot(a, b)
        numpy_time = (time.time() - start_time) / 100
        print(f"  NumPyè®¡ç®—é€Ÿåº¦: {numpy_time*1000:.1f} ms/çŸ©é˜µä¹˜æ³•")
    except:
        print("  NumPyæµ‹è¯•å¤±è´¥")
    
    # æœ€ç»ˆæ€»ç»“
    print("\n" + "=" * 60)
    print("æµ‹è¯•å®Œæˆï¼")
    print("=" * 60)
    
    # æä¾›å»ºè®®
    if len(available_cameras) == 0:
        print("å»ºè®®:")
        print("  1. è¿æ¥æ‘„åƒå¤´æˆ–ä½¿ç”¨è™šæ‹Ÿæ‘„åƒå¤´è½¯ä»¶")
        print("  2. è¿è¡Œå‘½ä»¤æµ‹è¯•æ‘„åƒå¤´: python main.py --list-cameras")
    
    if len(found_models) == 0:
        print("è­¦å‘Š:")
        print("  æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œç³»ç»Ÿå¯èƒ½æ— æ³•å·¥ä½œ")
        print("  è¯·ä¸‹è½½æ¨¡å‹æ–‡ä»¶åˆ°å½“å‰ç›®å½•:")
        print("  https://github.com/ultralytics/ultralytics")
    
    if len(found_images) == 0:
        print("æç¤º:")
        print("  å¯ä»¥æ”¾ç½®ä¸€äº›æµ‹è¯•å›¾ç‰‡åœ¨å½“å‰ç›®å½•")
        print("  æˆ–ä½¿ç”¨æ‘„åƒå¤´å®æ—¶æ£€æµ‹åŠŸèƒ½")
    
    print("\nç³»ç»Ÿå‡†å¤‡çŠ¶æ€:")
    status_ok = "âœ“" if len(found_models) > 0 else "âœ—"
    print(f"  {status_ok} æ¨¡å‹æ–‡ä»¶: {'å·²å‡†å¤‡' if len(found_models) > 0 else 'æœªå‡†å¤‡'}")
    
    status_ok = "âœ“" if len(available_cameras) > 0 else "âš "
    print(f"  {status_ok} æ‘„åƒå¤´: {f'æ‰¾åˆ° {len(available_cameras)} ä¸ª' if available_cameras else 'æœªæ‰¾åˆ°'}")
    
    status_ok = "âœ“" if len(found_images) > 0 else "âš "
    print(f"  {status_ok} æµ‹è¯•å›¾ç‰‡: {f'æ‰¾åˆ° {len(found_images)} ä¸ª' if found_images else 'æœªæ‰¾åˆ°'}")
    
    print("\nå¯ä»¥è¿è¡Œä»¥ä¸‹å‘½ä»¤å¼€å§‹:")
    if len(found_images) > 0:
        print(f"  python main.py --image {found_images[0]}")
    if len(available_cameras) > 0:
        print(f"  python main.py --camera --camera-index {available_cameras[0]['index']}")
    print("  python main.py  # è¿›å…¥äº¤äº’æ¨¡å¼")
    print("=" * 60)


if __name__ == "__main__":
    main()